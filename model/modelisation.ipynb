{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b9eb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "\n",
    "# Sauvegarde des mod√®les\n",
    "import joblib\n",
    "\n",
    "# Syst√®me et utilitaires\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# API et web\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Implicite \n",
    "import streamlit\n",
    "import requests\n",
    "import uvicorn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f0f18",
   "metadata": {},
   "source": [
    "# 5. VECTORISATION ET MOD√âLISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02822ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "combined_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cleaned_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "processed_text",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3fda8ef7-e6e6-4895-adaa-158c1422c344",
       "rows": [
        [
         "0",
         "0",
         "ablaze",
         null,
         "Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze‚Ä¶",
         "1",
         "Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze‚Ä¶ ablaze",
         "communal violenc bhainsa telangana stone pelt muslim hous hous vehicl set ablaz ablaz",
         "125",
         "communal violenc in bhainsa telangana stone were pelt on muslim hous and some hous and vehicl were set ablaz"
        ],
        [
         "1",
         "1",
         "ablaze",
         null,
         "Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po‚Ä¶",
         "1",
         "Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po‚Ä¶ ablaze",
         "telangana section impos bhainsa januari clash erupt two group januari po ablaz",
         "131",
         "telangana section has been impos in bhainsa from januari to after clash erupt between two group on januari po"
        ],
        [
         "2",
         "2",
         "ablaze",
         "New York City",
         "Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI",
         "1",
         "Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI ablaze",
         "arsonist set car ablaz dealership ablaz",
         "63",
         "arsonist set car ablaz at dealership"
        ],
        [
         "3",
         "4",
         "ablaze",
         null,
         "\"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l‚Ä¶ https://t.co/VlTznnPNi8",
         "0",
         "\"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l‚Ä¶ https://t.co/VlTznnPNi8 ablaze",
         "lord jesus love bring freedom pardon fill holi spirit set heart ablaz l ablaz",
         "140",
         "lord jesus your love bring freedom and pardon fill me with your holi spirit and set my heart ablaz with your l"
        ],
        [
         "4",
         "5",
         "ablaze",
         "OC",
         "If this child was Chinese, this tweet would have gone viral. Social media would be ablaze. SNL would have made a racist j‚Ä¶",
         "0",
         "If this child was Chinese, this tweet would have gone viral. Social media would be ablaze. SNL would have made a racist j‚Ä¶ ablaze",
         "child chines tweet would gone viral social media would ablaz snl would made racist j ablaz",
         "122",
         "if this child was chines this tweet would have gone viral social media would be ablaz snl would have made a racist j"
        ],
        [
         "5",
         "6",
         "ablaze",
         "London, England",
         "Several houses have been set ablaze in Ngemsibaa village, Oku sub division in the North West Region of Cameroon by‚Ä¶ https://t.co/99uHGAzxy2",
         "1",
         "Several houses have been set ablaze in Ngemsibaa village, Oku sub division in the North West Region of Cameroon by‚Ä¶ https://t.co/99uHGAzxy2 ablaze",
         "sever hous set ablaz ngemsibaa villag oku sub divis north west region cameroon ablaz",
         "139",
         "sever hous have been set ablaz in ngemsibaa villag oku sub divis in the north west region of cameroon by"
        ],
        [
         "6",
         "7",
         "ablaze",
         "Bharat",
         "Asansol: A BJP office in Salanpur village was set ablaze last night. BJP has alleged that TMC is behind the incident. Police has b‚Ä¶",
         "1",
         "Asansol: A BJP office in Salanpur village was set ablaze last night. BJP has alleged that TMC is behind the incident. Police has b‚Ä¶ ablaze",
         "asansol bjp offic salanpur villag set ablaz last night bjp alleg tmc behind incid polic b ablaz",
         "131",
         "asansol a bjp offic in salanpur villag was set ablaz last night bjp has alleg that tmc is behind the incid polic has b"
        ],
        [
         "7",
         "8",
         "ablaze",
         "Accra, Ghana",
         "National Security Minister, Kan Dapaah's side chic has set the internet ablaze with her latest powerful video.‚Ä¶ https://t.co/rhzOMQVSlj",
         "0",
         "National Security Minister, Kan Dapaah's side chic has set the internet ablaze with her latest powerful video.‚Ä¶ https://t.co/rhzOMQVSlj ablaze",
         "nation secur minist kan dapaah side chic set internet ablaz latest power video ablaz",
         "135",
         "nation secur minist kan dapaah s side chic has set the internet ablaz with her latest power video"
        ],
        [
         "8",
         "9",
         "ablaze",
         "Searching",
         "This creature who‚Äôs soul is no longer clarent but blue ablaze This thing Carrying memories Memories of‚Ä¶ https://t.co/tBKSNDrDoX",
         "0",
         "This creature who‚Äôs soul is no longer clarent but blue ablaze This thing Carrying memories Memories of‚Ä¶ https://t.co/tBKSNDrDoX ablaze",
         "creatur soul longer clarent blue ablaz thing carri memori memori ablaz",
         "127",
         "this creatur who is soul is no longer clarent but blue ablaz this thing carri memori memori of"
        ],
        [
         "9",
         "10",
         "ablaze",
         null,
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is reported‚Ä¶",
         "1",
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is reported‚Ä¶ ablaze",
         "imag show havoc caus cameroon militari torch hous oku shameless militari report ablaz",
         "124",
         "imag show the havoc caus by the cameroon militari as they torch hous in oku the shameless militari is report"
        ],
        [
         "10",
         "11",
         "ablaze",
         null,
         "Social media went bananas after Chuba Hubbard announced Monday evening his plans to return to #okstate. https://t.co/peN‚Ä¶",
         "0",
         "Social media went bananas after Chuba Hubbard announced Monday evening his plans to return to #okstate. https://t.co/peN‚Ä¶ ablaze",
         "social media went banana chuba hubbard announc monday even plan return okstat ablaz",
         "121",
         "social media went banana after chuba hubbard announc monday even his plan to return to okstat"
        ],
        [
         "11",
         "12",
         "ablaze",
         null,
         "Hausa youths set Area Office of Apapa-Iganmu Local Council Development Area ablaze. Okada Riders stormed the LG area office‚Ä¶",
         "1",
         "Hausa youths set Area Office of Apapa-Iganmu Local Council Development Area ablaze. Okada Riders stormed the LG area office‚Ä¶ ablaze",
         "hausa youth set area offic apapa iganmu local council develop area ablaz okada rider storm lg area offic ablaz",
         "124",
         "hausa youth set area offic of apapa iganmu local council develop area ablaz okada rider storm the lg area offic"
        ],
        [
         "12",
         "13",
         "ablaze",
         "HYDERABAD",
         "Under #MamataBanerjee political violence &amp; vandalism continues to unabated in West Bengal! office in Asanol was‚Ä¶",
         "1",
         "Under #MamataBanerjee political violence &amp; vandalism continues to unabated in West Bengal! office in Asanol was‚Ä¶ ablaze",
         "mamatabanerje polit violenc amp vandal continu unab west bengal offic asanol ablaz",
         "116",
         "under mamatabanerje polit violenc amp vandal continu to unab in west bengal offic in asanol was"
        ],
        [
         "13",
         "14",
         "ablaze",
         "Reno, NV",
         "AMEN! Set the whole system ablaze, man. https://t.co/J08xHDcGbD",
         "0",
         "AMEN! Set the whole system ablaze, man. https://t.co/J08xHDcGbD ablaze",
         "amen set whole system ablaz man ablaz",
         "63",
         "amen set the whole system ablaz man"
        ],
        [
         "14",
         "15",
         "ablaze",
         null,
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is‚Ä¶ https://t.co/gIwZCH533D",
         "1",
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is‚Ä¶ https://t.co/gIwZCH533D ablaze",
         "imag show havoc caus cameroon militari torch hous oku shameless militari ablaz",
         "139",
         "imag show the havoc caus by the cameroon militari as they torch hous in oku the shameless militari is"
        ],
        [
         "15",
         "16",
         "ablaze",
         null,
         "No cows today but our local factory is sadly still ablaze #REDJanuary2020 https://t.co/CMyuKzrcKz",
         "1",
         "No cows today but our local factory is sadly still ablaze #REDJanuary2020 https://t.co/CMyuKzrcKz ablaze",
         "cow today local factori sad still ablaz redjanuari ablaz",
         "97",
         "no cow today but our local factori is sad still ablaz redjanuari"
        ],
        [
         "16",
         "17",
         "ablaze",
         null,
         "Rengoku sets my heart ablazeüòî‚ù§Ô∏èüî• P.s. I missed this style of coloring I do so here it is c: #È¨ºÊªÖ„ÅÆÂàÉ https://t.co/YrUF9g68s0",
         "0",
         "Rengoku sets my heart ablazeüòî‚ù§Ô∏èüî• P.s. I missed this style of coloring I do so here it is c: #È¨ºÊªÖ„ÅÆÂàÉ https://t.co/YrUF9g68s0 ablaze",
         "rengoku set heart ablaz p miss style color c È¨ºÊªÖ„ÅÆÂàÉ ablaz",
         "121",
         "rengoku set my heart ablaz p s i miss this style of color i do so here it is c È¨ºÊªÖ„ÅÆÂàÉ"
        ],
        [
         "17",
         "18",
         "ablaze",
         "Worldwide",
         "paulzizkaphoto: ‚ÄúRundle Ablaze‚Äù Wishing you all a good evening... https://t.co/d0NlME1HQz https://t.co/hlVlT6qiIp",
         "0",
         "paulzizkaphoto: ‚ÄúRundle Ablaze‚Äù Wishing you all a good evening... https://t.co/d0NlME1HQz https://t.co/hlVlT6qiIp ablaze",
         "paulzizkaphoto rundl ablaz wish good even ablaz",
         "113",
         "paulzizkaphoto rundl ablaz wish you all a good even"
        ],
        [
         "18",
         "19",
         "ablaze",
         null,
         "French cameroun set houses ablaze in Ndu and roasted two young boys in their homes in #targeted killings in a #GenocideInSou‚Ä¶",
         "1",
         "French cameroun set houses ablaze in Ndu and roasted two young boys in their homes in #targeted killings in a #GenocideInSou‚Ä¶ ablaze",
         "french cameroun set hous ablaz ndu roast two young boy home target kill genocideinsou ablaz",
         "125",
         "french cameroun set hous ablaz in ndu and roast two young boy in their home in target kill in a genocideinsou"
        ],
        [
         "19",
         "20",
         "ablaze",
         null,
         "Cameroon's #BIR soldiers on the 05/01/2020 invaded the #SouthernCameroons Village of Kimar - So setting ablaze a total of‚Ä¶",
         "1",
         "Cameroon's #BIR soldiers on the 05/01/2020 invaded the #SouthernCameroons Village of Kimar - So setting ablaze a total of‚Ä¶ ablaze",
         "cameroon bir soldier invad southerncameroon villag kimar set ablaz total ablaz",
         "122",
         "cameroon s bir soldier on the invad the southerncameroon villag of kimar so set ablaz a total of"
        ],
        [
         "20",
         "21",
         "ablaze",
         null,
         "As fires ablaze throughout the land/as the prophosized apocalypse comes to fruition/not a building standing/as we all come st‚Ä¶",
         "1",
         "As fires ablaze throughout the land/as the prophosized apocalypse comes to fruition/not a building standing/as we all come st‚Ä¶ ablaze",
         "fire ablaz throughout land prophos apocalyps come fruition build stand come st ablaz",
         "126",
         "as fire ablaz throughout the land as the prophos apocalyps come to fruition not a build stand as we all come st"
        ],
        [
         "21",
         "22",
         "ablaze",
         "Italy",
         "#ThankfulTuesday Isaiah 43:2 When you pass through the waters, I will be with you; and when you pass through the r‚Ä¶ https://t.co/jEp3L1oU36",
         "0",
         "#ThankfulTuesday Isaiah 43:2 When you pass through the waters, I will be with you; and when you pass through the r‚Ä¶ https://t.co/jEp3L1oU36 ablaze",
         "thankfultuesday isaiah pass water pass r ablaz",
         "139",
         "thankfultuesday isaiah when you pass through the water i will be with you and when you pass through the r"
        ],
        [
         "22",
         "23",
         "ablaze",
         "` ÀóÀèÀã i'm‚†Äwaiting‚†Äfor‚†Äyou‚†Äto‚†Äpour‚†Ämy‚†ÄùòÄùó∂ùóªùòÄ‚†Äonto‚†Äme‚†Ä;‚†Äcall‚†Äme‚†Äyour‚†Äùò≠ùò∞ùò∑ùò¶ùò≥‚†Äwhispering‚†Ähymns‚†Äto‚†Ämy‚†Äear‚†Ä,‚†Äcrucify‚†Äme‚†Äin‚†Äclaims‚†Äof‚†Äpraising‚†Äme .",
         "‚†Ä‚†ÄWhen you walk through the fire, ‚†Ä‚†Äyou will not be scorched, and the ‚†Ä‚†Äflames will not set you ablaze. ‚îÄ‚îÄ‚îÄ‚îÄ product‚Ä¶",
         "0",
         "‚†Ä‚†ÄWhen you walk through the fire, ‚†Ä‚†Äyou will not be scorched, and the ‚†Ä‚†Äflames will not set you ablaze. ‚îÄ‚îÄ‚îÄ‚îÄ product‚Ä¶ ablaze",
         "walk fire scorch flame set ablaz product ablaz",
         "117",
         "when you walk through the fire you will not be scorch and the flame will not set you ablaz product"
        ],
        [
         "23",
         "24",
         "ablaze",
         "Okielahoma",
         "Originally they were intended to be fired at boarders or the opposing ship's crew from gunners in the ships's riggi‚Ä¶ https://t.co/z5vPwuyBCL",
         "1",
         "Originally they were intended to be fired at boarders or the opposing ship's crew from gunners in the ships's riggi‚Ä¶ https://t.co/z5vPwuyBCL ablaze",
         "origin intend fire boarder oppos ship crew gunner ship riggi ablaz",
         "140",
         "origin they were intend to be fire at boarder or the oppos ship s crew from gunner in the ship s riggi"
        ],
        [
         "24",
         "25",
         "ablaze",
         null,
         "Warm greetings to all on the occasion of #Lohri. As winter passes by may everyone's woes and troubles be set ablaze in t‚Ä¶",
         "0",
         "Warm greetings to all on the occasion of #Lohri. As winter passes by may everyone's woes and troubles be set ablaze in t‚Ä¶ ablaze",
         "warm greet occas lohri winter pass may everyon woe troubl set ablaz ablaz",
         "121",
         "warm greet to all on the occas of lohri as winter pass by may everyon is woe and troubl be set ablaz in t"
        ],
        [
         "25",
         "26",
         "ablaze",
         null,
         "Another arson in Njikom,Boyo,NWR. The ambazombies yesterday 13/1/2020 set ablaze the Council building. Which country in the wo‚Ä¶",
         "1",
         "Another arson in Njikom,Boyo,NWR. The ambazombies yesterday 13/1/2020 set ablaze the Council building. Which country in the wo‚Ä¶ ablaze",
         "anoth arson njikom boyo nwr ambazombi yesterday set ablaz council build countri wo ablaz",
         "127",
         "anoth arson in njikom boyo nwr the ambazombi yesterday set ablaz the council build which countri in the wo"
        ],
        [
         "26",
         "27",
         "ablaze",
         "Havana 3am",
         "Another public market in #Haiti mysteriously set ablaze. The independent merchants, who are overwhelmingly women, become‚Ä¶",
         "1",
         "Another public market in #Haiti mysteriously set ablaze. The independent merchants, who are overwhelmingly women, become‚Ä¶ ablaze",
         "anoth public market haiti mysteri set ablaz independ merchant overwhelm women becom ablaz",
         "121",
         "anoth public market in haiti mysteri set ablaz the independ merchant who are overwhelm women becom"
        ],
        [
         "27",
         "28",
         "ablaze",
         null,
         "that is kind true sadly",
         "0",
         "that is kind true sadly ablaze",
         "kind true sad ablaz",
         "23",
         "that is kind true sad"
        ],
        [
         "28",
         "29",
         "ablaze",
         null,
         "I swear that jam will set the world ablaze",
         "0",
         "I swear that jam will set the world ablaze ablaze",
         "swear jam set world ablaz ablaz",
         "42",
         "i swear that jam will set the world ablaz"
        ],
        [
         "29",
         "30",
         "ablaze",
         null,
         "Marivan, Kurdistan Province Monday, Jan 13th, 2020 Protesters set the propaganda banner of #QassemSoleimani ablaze. #I‚Ä¶",
         "1",
         "Marivan, Kurdistan Province Monday, Jan 13th, 2020 Protesters set the propaganda banner of #QassemSoleimani ablaze. #I‚Ä¶ ablaze",
         "marivan kurdistan provinc monday jan th protest set propaganda banner qassemsoleimani ablaz ablaz",
         "119",
         "marivan kurdistan provinc monday jan th protest set the propaganda banner of qassemsoleimani ablaz i"
        ],
        [
         "30",
         "32",
         "ablaze",
         "India",
         "How can you turn a blind eye to the icident of setting ablaze more than 50 houses of H‚Ä¶ https://t.co/UD84oQxg9K",
         "1",
         "How can you turn a blind eye to the icident of setting ablaze more than 50 houses of H‚Ä¶ https://t.co/UD84oQxg9K ablaze",
         "turn blind eye icid set ablaz hous h ablaz",
         "111",
         "how can you turn a blind eye to the icid of set ablaz more than hous of h"
        ],
        [
         "31",
         "33",
         "ablaze",
         "Salta, Argentina",
         "This love is so completely crazy. You've been fucking with my dreams, ripped me like your torn up jeans. I don't ev‚Ä¶ https://t.co/GCVYW5eZkb",
         "0",
         "This love is so completely crazy. You've been fucking with my dreams, ripped me like your torn up jeans. I don't ev‚Ä¶ https://t.co/GCVYW5eZkb ablaze",
         "love complet crazi fuck dream rip like torn jean ev ablaz",
         "140",
         "this love is so complet crazi you have been fuck with my dream rip me like your torn up jean i do not ev"
        ],
        [
         "32",
         "34",
         "accident",
         "Wherever socks go in the dryer",
         "Terms in A Demon Burning Dark: The Ruined: People who cannot use magic or interact with it without some harm or ac‚Ä¶ https://t.co/ZEDawOfuu4",
         "0",
         "Terms in A Demon Burning Dark: The Ruined: People who cannot use magic or interact with it without some harm or ac‚Ä¶ https://t.co/ZEDawOfuu4 accident",
         "term demon burn dark ruin peopl use magic interact without harm ac accid",
         "139",
         "term in a demon burn dark the ruin peopl who can not use magic or interact with it without some harm or ac"
        ],
        [
         "33",
         "35",
         "accident",
         "Kuala Lumpur",
         "üì∑ Heartfelt appreciation to Prime Minister YAB Tun Dr. wife, YABhg. Tun Dr. Siti Hasmah Mohd Ali fo‚Ä¶ https://t.co/YOwUp1BYUP",
         "0",
         "üì∑ Heartfelt appreciation to Prime Minister YAB Tun Dr. wife, YABhg. Tun Dr. Siti Hasmah Mohd Ali fo‚Ä¶ https://t.co/YOwUp1BYUP accident",
         "heartfelt appreci prime minist yab tun dr wife yabhg tun dr siti hasmah mohd ali fo accid",
         "124",
         "heartfelt appreci to prime minist yab tun dr wife yabhg tun dr siti hasmah mohd ali fo"
        ],
        [
         "34",
         "36",
         "accident",
         null,
         "#WATCH Former CM Akhilesh Yadav who went to meet injured of Kannauj accident, at a hospital in Chhibramau asks Emergency Med‚Ä¶",
         "1",
         "#WATCH Former CM Akhilesh Yadav who went to meet injured of Kannauj accident, at a hospital in Chhibramau asks Emergency Med‚Ä¶ accident",
         "watch former cm akhilesh yadav went meet injur kannauj accid hospit chhibramau ask emerg med accid",
         "125",
         "watch former cm akhilesh yadav who went to meet injur of kannauj accid at a hospit in chhibramau ask emerg med"
        ],
        [
         "35",
         "37",
         "accident",
         "Sydney, New South Wales",
         "‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è he gave us everything... He had a horrible foot infection once so wore one thong‚Ä¶ https://t.co/mA9sFl6Shw",
         "0",
         "‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è he gave us everything... He had a horrible foot infection once so wore one thong‚Ä¶ https://t.co/mA9sFl6Shw accident",
         "gave us everyth horribl foot infect wore one thong accid",
         "112",
         "he gave us everyth he had a horribl foot infect onc so wore one thong"
        ],
        [
         "36",
         "38",
         "accident",
         "Nigeria",
         "üòÅyeah! His new swag is on point 100%, since the accident! Like this is a totally transformed Bob‚Ä¶",
         "0",
         "üòÅyeah! His new swag is on point 100%, since the accident! Like this is a totally transformed Bob‚Ä¶ accident",
         "yeah new swag point sinc accid like total transform bob accid",
         "97",
         "yeah his new swag is on point sinc the accid like this is a total transform bob"
        ],
        [
         "37",
         "39",
         "accident",
         "Davanagere, India ",
         "This is cool and all these days I have been doing \"git push origin CURRENT_BRANCH_NAME\". You know that‚Ä¶ https://t.co/mr0YAGEWqj",
         "0",
         "This is cool and all these days I have been doing \"git push origin CURRENT_BRANCH_NAME\". You know that‚Ä¶ https://t.co/mr0YAGEWqj accident",
         "cool day git push origin current_branch_nam know accid",
         "127",
         "this is cool and all these day i have been do git push origin current_branch_nam you know that"
        ],
        [
         "38",
         "40",
         "accident",
         "Lyndhurst, OH",
         "#Preorder #newrelease today! 12 witnesses connected to or investigating #THENUTCRACKERCONSPIRACY have died either in a‚Ä¶",
         "1",
         "#Preorder #newrelease today! 12 witnesses connected to or investigating #THENUTCRACKERCONSPIRACY have died either in a‚Ä¶ accident",
         "preorder newreleas today wit connect investig thenutcrackerconspiraci die either accid",
         "119",
         "preorder newreleas today wit connect to or investig thenutcrackerconspiraci have die either in a"
        ],
        [
         "39",
         "41",
         "accident",
         "Covina, Ca",
         "my back and neck are still fucked up from the accident üò°üò°üò≠üò≠",
         "0",
         "my back and neck are still fucked up from the accident üò°üò°üò≠üò≠ accident",
         "back neck still fuck accid accid",
         "59",
         "my back and neck are still fuck up from the accid"
        ],
        [
         "40",
         "42",
         "accident",
         "Colorado, Brighton",
         "RT! Prince Harry just confirmed that his mother‚Äôs (Princess Diana) death was not an accident! https://t.co/1ADe3uZ3eR",
         "1",
         "RT! Prince Harry just confirmed that his mother‚Äôs (Princess Diana) death was not an accident! https://t.co/1ADe3uZ3eR accident",
         "rt princ harri confirm mother princess diana death accid accid",
         "117",
         "rt princ harri just confirm that his mother s princess diana death was not an accid"
        ],
        [
         "41",
         "43",
         "accident",
         null,
         "Note to Democrats: It‚Äôs not a Muslim ban. Islam is not a race. Soleimani was a terrorist &amp; was exterminated not assas‚Ä¶",
         "0",
         "Note to Democrats: It‚Äôs not a Muslim ban. Islam is not a race. Soleimani was a terrorist &amp; was exterminated not assas‚Ä¶ accident",
         "note democrat muslim ban islam race soleimani terrorist amp extermin assa accid",
         "122",
         "note to democrat it is not a muslim ban islam is not a race soleimani was a terrorist amp was extermin not assa"
        ],
        [
         "42",
         "44",
         "accident",
         "North Carolina, USA",
         "Juwan Johnson/Oregon is one big dude. Looks like a tight end stuck in the receiver group by accident.",
         "0",
         "Juwan Johnson/Oregon is one big dude. Looks like a tight end stuck in the receiver group by accident. accident",
         "juwan johnson oregon one big dude look like tight end stuck receiv group accid accid",
         "101",
         "juwan johnson oregon is one big dude look like a tight end stuck in the receiv group by accid"
        ],
        [
         "43",
         "45",
         "accident",
         "kurger bing",
         "More appearances of the man with the upside-down face. A New Year‚Äôs Eve party at an Air Force base in 1943 where a man‚Ä¶",
         "0",
         "More appearances of the man with the upside-down face. A New Year‚Äôs Eve party at an Air Force base in 1943 where a man‚Ä¶ accident",
         "appear man upsid face new year eve parti air forc base man accid",
         "119",
         "more appear of the man with the upsid down face a new year s eve parti at an air forc base in where a man"
        ],
        [
         "44",
         "46",
         "accident",
         "Faridabad, India",
         "The speeding car rammed into a group of people, who were returning after attending a temple festival of Ayyappan Kavu in Thum‚Ä¶",
         "1",
         "The speeding car rammed into a group of people, who were returning after attending a temple festival of Ayyappan Kavu in Thum‚Ä¶ accident",
         "speed car ram group peopl return attend templ festiv ayyappan kavu thum accid",
         "126",
         "the speed car ram into a group of peopl who were return after attend a templ festiv of ayyappan kavu in thum"
        ],
        [
         "45",
         "47",
         "accident",
         "60",
         "My friend (an army) just lost her father in an accident and her mom right now is still unconscious. Please pray for her mo‚Ä¶",
         "1",
         "My friend (an army) just lost her father in an accident and her mom right now is still unconscious. Please pray for her mo‚Ä¶ accident",
         "friend armi lost father accid mom right still unconsci pleas pray mo accid",
         "123",
         "my friend an armi just lost her father in an accid and her mom right now is still unconsci pleas pray for her mo"
        ],
        [
         "46",
         "48",
         "accident",
         "ngovhela Mahunguni",
         "MLINDO THE VOCALIST IN ANOTHER CAR ACCIDENT https://t.co/BXR9rEgAk6",
         "1",
         "MLINDO THE VOCALIST IN ANOTHER CAR ACCIDENT https://t.co/BXR9rEgAk6 accident",
         "mlindo vocalist anoth car accid accid",
         "67",
         "mlindo the vocalist in anoth car accid"
        ],
        [
         "47",
         "49",
         "accident",
         null,
         "‚ÄúThere are no greater treasures than the highest human qualities such as compassion, courage and hope. Not even tragic accide‚Ä¶",
         "0",
         "‚ÄúThere are no greater treasures than the highest human qualities such as compassion, courage and hope. Not even tragic accide‚Ä¶ accident",
         "greater treasur highest human qualiti compass courag hope even tragic accid accid",
         "126",
         "there are no greater treasur than the highest human qualiti such as compass courag and hope not even tragic accid"
        ],
        [
         "48",
         "50",
         "accident",
         "Germany",
         "Please help our friends in - they have had a non fault accident that's resulted in their vehicle being writte‚Ä¶",
         "0",
         "Please help our friends in - they have had a non fault accident that's resulted in their vehicle being writte‚Ä¶ accident",
         "pleas help friend non fault accid result vehicl writt accid",
         "110",
         "pleas help our friend in they have had a non fault accid that is result in their vehicl be writt"
        ],
        [
         "49",
         "51",
         "accident",
         "South Beach, FL",
         "When you hurt your younger sibling by ‚Äúaccident‚Äù https://t.co/DAMTEoQtZU",
         "0",
         "When you hurt your younger sibling by ‚Äúaccident‚Äù https://t.co/DAMTEoQtZU accident",
         "hurt younger sibl accid accid",
         "72",
         "when you hurt your younger sibl by accid"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10803
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>communal violenc bhainsa telangana stone pelt ...</td>\n",
       "      <td>125</td>\n",
       "      <td>communal violenc in bhainsa telangana stone we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>telangana section impos bhainsa januari clash ...</td>\n",
       "      <td>131</td>\n",
       "      <td>telangana section has been impos in bhainsa fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>arsonist set car ablaz dealership ablaz</td>\n",
       "      <td>63</td>\n",
       "      <td>arsonist set car ablaz at dealership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>lord jesus love bring freedom pardon fill holi...</td>\n",
       "      <td>140</td>\n",
       "      <td>lord jesus your love bring freedom and pardon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>If this child was Chinese, this tweet would ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>If this child was Chinese, this tweet would ha...</td>\n",
       "      <td>child chines tweet would gone viral social med...</td>\n",
       "      <td>122</td>\n",
       "      <td>if this child was chines this tweet would have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10798</th>\n",
       "      <td>11364</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Had these guys last game n fcked them. Talked ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Had these guys last game n fcked them. Talked ...</td>\n",
       "      <td>guy last game n fcked talk non stop shit n sti...</td>\n",
       "      <td>139</td>\n",
       "      <td>had these guy last game n fcked them talk non ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>11365</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Blue State in a red sea</td>\n",
       "      <td>Media should have warned us well in advance. T...</td>\n",
       "      <td>0</td>\n",
       "      <td>Media should have warned us well in advance. T...</td>\n",
       "      <td>media warn us well advanc wreck whole night re...</td>\n",
       "      <td>92</td>\n",
       "      <td>media should have warn us well in advanc this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>11366</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>arohaonces</td>\n",
       "      <td>i feel directly attacked üíÄ i consider moonbin ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i feel directly attacked üíÄ i consider moonbin ...</td>\n",
       "      <td>feel direct attack consid moonbin amp jinjin b...</td>\n",
       "      <td>115</td>\n",
       "      <td>i feel direct attack i consid moonbin amp jinj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10801</th>\n",
       "      <td>11368</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>auroraborealis</td>\n",
       "      <td>ok who remember \"outcast\" nd the \"dora\" au?? T...</td>\n",
       "      <td>0</td>\n",
       "      <td>ok who remember \"outcast\" nd the \"dora\" au?? T...</td>\n",
       "      <td>ok rememb outcast nd dora au au wreck nerv nd ...</td>\n",
       "      <td>105</td>\n",
       "      <td>ok who rememb outcast nd the dora au those au ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>11369</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jake Corway wrecked while running 14th at IRP.</td>\n",
       "      <td>1</td>\n",
       "      <td>Jake Corway wrecked while running 14th at IRP....</td>\n",
       "      <td>jake corway wreck run th irp wreck</td>\n",
       "      <td>46</td>\n",
       "      <td>jake corway wreck while run th at irp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10803 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  keyword                 location  \\\n",
       "0          0   ablaze                      NaN   \n",
       "1          1   ablaze                      NaN   \n",
       "2          2   ablaze            New York City   \n",
       "3          4   ablaze                      NaN   \n",
       "4          5   ablaze                       OC   \n",
       "...      ...      ...                      ...   \n",
       "10798  11364  wrecked                      NaN   \n",
       "10799  11365  wrecked  Blue State in a red sea   \n",
       "10800  11366  wrecked               arohaonces   \n",
       "10801  11368  wrecked           auroraborealis   \n",
       "10802  11369  wrecked                      NaN   \n",
       "\n",
       "                                                    text  target  \\\n",
       "0      Communal violence in Bhainsa, Telangana. \"Ston...       1   \n",
       "1      Telangana: Section 144 has been imposed in Bha...       1   \n",
       "2      Arsonist sets cars ablaze at dealership https:...       1   \n",
       "3      \"Lord Jesus, your love brings freedom and pard...       0   \n",
       "4      If this child was Chinese, this tweet would ha...       0   \n",
       "...                                                  ...     ...   \n",
       "10798  Had these guys last game n fcked them. Talked ...       0   \n",
       "10799  Media should have warned us well in advance. T...       0   \n",
       "10800  i feel directly attacked üíÄ i consider moonbin ...       0   \n",
       "10801  ok who remember \"outcast\" nd the \"dora\" au?? T...       0   \n",
       "10802     Jake Corway wrecked while running 14th at IRP.       1   \n",
       "\n",
       "                                           combined_text  \\\n",
       "0      Communal violence in Bhainsa, Telangana. \"Ston...   \n",
       "1      Telangana: Section 144 has been imposed in Bha...   \n",
       "2      Arsonist sets cars ablaze at dealership https:...   \n",
       "3      \"Lord Jesus, your love brings freedom and pard...   \n",
       "4      If this child was Chinese, this tweet would ha...   \n",
       "...                                                  ...   \n",
       "10798  Had these guys last game n fcked them. Talked ...   \n",
       "10799  Media should have warned us well in advance. T...   \n",
       "10800  i feel directly attacked üíÄ i consider moonbin ...   \n",
       "10801  ok who remember \"outcast\" nd the \"dora\" au?? T...   \n",
       "10802  Jake Corway wrecked while running 14th at IRP....   \n",
       "\n",
       "                                            cleaned_text  text_length  \\\n",
       "0      communal violenc bhainsa telangana stone pelt ...          125   \n",
       "1      telangana section impos bhainsa januari clash ...          131   \n",
       "2                arsonist set car ablaz dealership ablaz           63   \n",
       "3      lord jesus love bring freedom pardon fill holi...          140   \n",
       "4      child chines tweet would gone viral social med...          122   \n",
       "...                                                  ...          ...   \n",
       "10798  guy last game n fcked talk non stop shit n sti...          139   \n",
       "10799  media warn us well advanc wreck whole night re...           92   \n",
       "10800  feel direct attack consid moonbin amp jinjin b...          115   \n",
       "10801  ok rememb outcast nd dora au au wreck nerv nd ...          105   \n",
       "10802                 jake corway wreck run th irp wreck           46   \n",
       "\n",
       "                                          processed_text  \n",
       "0      communal violenc in bhainsa telangana stone we...  \n",
       "1      telangana section has been impos in bhainsa fr...  \n",
       "2                   arsonist set car ablaz at dealership  \n",
       "3      lord jesus your love bring freedom and pardon ...  \n",
       "4      if this child was chines this tweet would have...  \n",
       "...                                                  ...  \n",
       "10798  had these guy last game n fcked them talk non ...  \n",
       "10799  media should have warn us well in advanc this ...  \n",
       "10800  i feel direct attack i consid moonbin amp jinj...  \n",
       "10801  ok who rememb outcast nd the dora au those au ...  \n",
       "10802              jake corway wreck while run th at irp  \n",
       "\n",
       "[10803 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des donn√©es processed\n",
    "df_processed = pd.read_csv('data/processed/tweets_processed.csv')\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b1f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration des donn√©es en features et target\n",
    "X = df_processed['processed_text']\n",
    "y = df_processed['target']\n",
    "# S√©paration des donn√©es en train et test\n",
    "X_train, X_test = train_test_split(df_processed, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def258c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test du mod√®le bert-base-multilingual-cased (embedding par: mean) ===\n",
      "Cr√©ation des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings bert-base-multilingual-cased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8642/8642 [13:31<00:00, 10.65it/s]\n",
      "Embeddings bert-base-multilingual-cased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2161/2161 [02:52<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entra√Ænement du mod√®le LogisticRegression...\n",
      "R√©sultats pour LogisticRegression:\n",
      "Exactitude: 0.8112\n",
      "Recall (g√©n√©ral): 0.8112\n",
      "F1-score (g√©n√©ral): 0.8260\n",
      "Recall (classe 1): 0.7507\n",
      "F1-score (classe 1): 0.5837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.88      1780\n",
      "           1       0.48      0.75      0.58       381\n",
      "\n",
      "    accuracy                           0.81      2161\n",
      "   macro avg       0.71      0.79      0.73      2161\n",
      "weighted avg       0.86      0.81      0.83      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le SVM...\n",
      "R√©sultats pour SVM:\n",
      "Exactitude: 0.8316\n",
      "Recall (g√©n√©ral): 0.8316\n",
      "F1-score (g√©n√©ral): 0.8442\n",
      "Recall (classe 1): 0.7900\n",
      "F1-score (classe 1): 0.6232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89      1780\n",
      "           1       0.51      0.79      0.62       381\n",
      "\n",
      "    accuracy                           0.83      2161\n",
      "   macro avg       0.73      0.82      0.76      2161\n",
      "weighted avg       0.87      0.83      0.84      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:02:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour XGBoost:\n",
      "Exactitude: 0.8760\n",
      "Recall (g√©n√©ral): 0.8760\n",
      "F1-score (g√©n√©ral): 0.8615\n",
      "Recall (classe 1): 0.4304\n",
      "F1-score (classe 1): 0.5503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      1780\n",
      "           1       0.76      0.43      0.55       381\n",
      "\n",
      "    accuracy                           0.88      2161\n",
      "   macro avg       0.83      0.70      0.74      2161\n",
      "weighted avg       0.87      0.88      0.86      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour LightGBM:\n",
      "Exactitude: 0.8561\n",
      "Recall (g√©n√©ral): 0.8561\n",
      "F1-score (g√©n√©ral): 0.8562\n",
      "Recall (classe 1): 0.5932\n",
      "F1-score (classe 1): 0.5924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      1780\n",
      "           1       0.59      0.59      0.59       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.86      0.86      0.86      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: SVM avec un score de 0.6232\n",
      "\n",
      "=== Test du mod√®le bert-base-multilingual-cased (embedding par: cls) ===\n",
      "Cr√©ation des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings bert-base-multilingual-cased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8642/8642 [11:54<00:00, 12.09it/s]\n",
      "Embeddings bert-base-multilingual-cased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2161/2161 [02:51<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entra√Ænement du mod√®le LogisticRegression...\n",
      "R√©sultats pour LogisticRegression:\n",
      "Exactitude: 0.7885\n",
      "Recall (g√©n√©ral): 0.7885\n",
      "F1-score (g√©n√©ral): 0.8057\n",
      "Recall (classe 1): 0.7008\n",
      "F1-score (classe 1): 0.5388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.86      1780\n",
      "           1       0.44      0.70      0.54       381\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.68      0.75      0.70      2161\n",
      "weighted avg       0.84      0.79      0.81      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le SVM...\n",
      "R√©sultats pour SVM:\n",
      "Exactitude: 0.7881\n",
      "Recall (g√©n√©ral): 0.7881\n",
      "F1-score (g√©n√©ral): 0.8061\n",
      "Recall (classe 1): 0.7244\n",
      "F1-score (classe 1): 0.5465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86      1780\n",
      "           1       0.44      0.72      0.55       381\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.69      0.76      0.70      2161\n",
      "weighted avg       0.84      0.79      0.81      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:19:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour XGBoost:\n",
      "Exactitude: 0.8616\n",
      "Recall (g√©n√©ral): 0.8616\n",
      "F1-score (g√©n√©ral): 0.8396\n",
      "Recall (classe 1): 0.3360\n",
      "F1-score (classe 1): 0.4613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92      1780\n",
      "           1       0.74      0.34      0.46       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.80      0.66      0.69      2161\n",
      "weighted avg       0.85      0.86      0.84      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour LightGBM:\n",
      "Exactitude: 0.8607\n",
      "Recall (g√©n√©ral): 0.8607\n",
      "F1-score (g√©n√©ral): 0.8555\n",
      "Recall (classe 1): 0.5197\n",
      "F1-score (classe 1): 0.5681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92      1780\n",
      "           1       0.63      0.52      0.57       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.76      0.73      0.74      2161\n",
      "weighted avg       0.85      0.86      0.86      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: LightGBM avec un score de 0.5681\n",
      "\n",
      "=== Test du mod√®le distilbert-base-uncased (embedding par: mean) ===\n",
      "Cr√©ation des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings distilbert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8642/8642 [05:36<00:00, 25.69it/s]\n",
      "Embeddings distilbert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2161/2161 [01:24<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entra√Ænement du mod√®le LogisticRegression...\n",
      "R√©sultats pour LogisticRegression:\n",
      "Exactitude: 0.8283\n",
      "Recall (g√©n√©ral): 0.8283\n",
      "F1-score (g√©n√©ral): 0.8412\n",
      "Recall (classe 1): 0.7795\n",
      "F1-score (classe 1): 0.6155\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89      1780\n",
      "           1       0.51      0.78      0.62       381\n",
      "\n",
      "    accuracy                           0.83      2161\n",
      "   macro avg       0.73      0.81      0.75      2161\n",
      "weighted avg       0.87      0.83      0.84      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le SVM...\n",
      "R√©sultats pour SVM:\n",
      "Exactitude: 0.8431\n",
      "Recall (g√©n√©ral): 0.8431\n",
      "F1-score (g√©n√©ral): 0.8538\n",
      "Recall (classe 1): 0.7874\n",
      "F1-score (classe 1): 0.6390\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      1780\n",
      "           1       0.54      0.79      0.64       381\n",
      "\n",
      "    accuracy                           0.84      2161\n",
      "   macro avg       0.74      0.82      0.77      2161\n",
      "weighted avg       0.88      0.84      0.85      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:28:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour XGBoost:\n",
      "Exactitude: 0.8792\n",
      "Recall (g√©n√©ral): 0.8792\n",
      "F1-score (g√©n√©ral): 0.8708\n",
      "Recall (classe 1): 0.5118\n",
      "F1-score (classe 1): 0.5991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93      1780\n",
      "           1       0.72      0.51      0.60       381\n",
      "\n",
      "    accuracy                           0.88      2161\n",
      "   macro avg       0.81      0.73      0.76      2161\n",
      "weighted avg       0.87      0.88      0.87      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour LightGBM:\n",
      "Exactitude: 0.8741\n",
      "Recall (g√©n√©ral): 0.8741\n",
      "F1-score (g√©n√©ral): 0.8755\n",
      "Recall (classe 1): 0.6719\n",
      "F1-score (classe 1): 0.6531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1780\n",
      "           1       0.64      0.67      0.65       381\n",
      "\n",
      "    accuracy                           0.87      2161\n",
      "   macro avg       0.78      0.79      0.79      2161\n",
      "weighted avg       0.88      0.87      0.88      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: LightGBM avec un score de 0.6531\n",
      "\n",
      "=== Test du mod√®le distilbert-base-uncased (embedding par: cls) ===\n",
      "Cr√©ation des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings distilbert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8642/8642 [06:10<00:00, 23.33it/s]\n",
      "Embeddings distilbert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2161/2161 [01:29<00:00, 24.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entra√Ænement du mod√®le LogisticRegression...\n",
      "R√©sultats pour LogisticRegression:\n",
      "Exactitude: 0.8390\n",
      "Recall (g√©n√©ral): 0.8390\n",
      "F1-score (g√©n√©ral): 0.8492\n",
      "Recall (classe 1): 0.7559\n",
      "F1-score (classe 1): 0.6234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90      1780\n",
      "           1       0.53      0.76      0.62       381\n",
      "\n",
      "    accuracy                           0.84      2161\n",
      "   macro avg       0.74      0.81      0.76      2161\n",
      "weighted avg       0.87      0.84      0.85      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le SVM...\n",
      "R√©sultats pour SVM:\n",
      "Exactitude: 0.8366\n",
      "Recall (g√©n√©ral): 0.8366\n",
      "F1-score (g√©n√©ral): 0.8479\n",
      "Recall (classe 1): 0.7717\n",
      "F1-score (classe 1): 0.6249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90      1780\n",
      "           1       0.53      0.77      0.62       381\n",
      "\n",
      "    accuracy                           0.84      2161\n",
      "   macro avg       0.74      0.81      0.76      2161\n",
      "weighted avg       0.87      0.84      0.85      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:38:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour XGBoost:\n",
      "Exactitude: 0.8783\n",
      "Recall (g√©n√©ral): 0.8783\n",
      "F1-score (g√©n√©ral): 0.8698\n",
      "Recall (classe 1): 0.5092\n",
      "F1-score (classe 1): 0.5960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93      1780\n",
      "           1       0.72      0.51      0.60       381\n",
      "\n",
      "    accuracy                           0.88      2161\n",
      "   macro avg       0.81      0.73      0.76      2161\n",
      "weighted avg       0.87      0.88      0.87      2161\n",
      "\n",
      "\n",
      "Entra√Ænement du mod√®le LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©sultats pour LightGBM:\n",
      "Exactitude: 0.8649\n",
      "Recall (g√©n√©ral): 0.8649\n",
      "F1-score (g√©n√©ral): 0.8654\n",
      "Recall (classe 1): 0.6273\n",
      "F1-score (classe 1): 0.6208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      1780\n",
      "           1       0.61      0.63      0.62       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.77      0.77      0.77      2161\n",
      "weighted avg       0.87      0.86      0.87      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: SVM avec un score de 0.6249\n",
      "\n",
      "=== R√©sum√© des r√©sultats ===\n",
      "                    Mod√®le BERT Strat√©gie Classifieur  Recall (classe 1)  \\\n",
      "2       distilbert-base-uncased      mean    LightGBM           0.671916   \n",
      "3       distilbert-base-uncased       cls         SVM           0.771654   \n",
      "0  bert-base-multilingual-cased      mean         SVM           0.790026   \n",
      "1  bert-base-multilingual-cased       cls    LightGBM           0.519685   \n",
      "\n",
      "   F1 (classe 1)     Score  \n",
      "2       0.653061  0.653061  \n",
      "3       0.624867  0.624867  \n",
      "0       0.623188  0.623188  \n",
      "1       0.568149  0.568149  \n",
      "\n",
      "Meilleure configuration: distilbert-base-uncased_mean avec LightGBM\n",
      "\n",
      "Cr√©ation des embeddings pour l'optimisation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings distilbert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8642/8642 [06:14<00:00, 23.09it/s]\n",
      "Embeddings distilbert-base-uncased: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2161/2161 [01:34<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimisation du classifieur LightGBM ===\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Meilleurs param√®tres: {'learning_rate': 0.3, 'n_estimators': 200, 'num_leaves': 50}\n",
      "\n",
      "R√©sultats apr√®s optimisation:\n",
      "Exactitude: 0.8903\n",
      "Recall (classe 1): 0.5669\n",
      "F1-score (classe 1): 0.6457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      1780\n",
      "           1       0.75      0.57      0.65       381\n",
      "\n",
      "    accuracy                           0.89      2161\n",
      "   macro avg       0.83      0.76      0.79      2161\n",
      "weighted avg       0.88      0.89      0.88      2161\n",
      "\n",
      "\n",
      "Exportation du mod√®le au format pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le export√©: bert_model.pkl\n"
     ]
    }
   ],
   "source": [
    "class BERTEmbedder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Classe pour transformer des textes en embeddings BERT\n",
    "    Compatible avec les pipelines sklearn\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='bert-base-multilingual-cased', max_length=128, embedding_strategy=\"cls\"):\n",
    "        \"\"\"\n",
    "        Initialise la classe BERTEmbedder\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nom du mod√®le √† utiliser\n",
    "            max_length (int): Longueur maximale des s√©quences apr√®s tokenization\n",
    "            embedding_strategy (str): Strat√©gie d'embedding (\"cls\" ou \"mean\")\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.embedding_strategy = embedding_strategy\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        valid_emb_strategy = (\"cls\", \"mean\")\n",
    "        if self.embedding_strategy not in valid_emb_strategy:\n",
    "            raise ValueError(f\"embedding_strategy doit √™tre l'un des suivants: {valid_emb_strategy}\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Charge le mod√®le et le tokenizer\n",
    "        Args:\n",
    "            X: Les textes d'entr√©e\n",
    "            y: Les labels\n",
    "            \n",
    "        Returns:\n",
    "            self: Retourne l'instance pour le cha√Ænage\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.model.eval()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforme les textes en embeddings\n",
    "        \n",
    "        Args:\n",
    "            X: Liste de textes √† transformer\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Matrice des embeddings\n",
    "        \"\"\"\n",
    "        # Convertir en liste si n√©cessaire\n",
    "        if isinstance(X, pd.Series) or isinstance(X, np.ndarray):\n",
    "            X = X.tolist()\n",
    "        elif not isinstance(X, list):\n",
    "            raise ValueError(\"Les donn√©es d'entr√©e doivent √™tre une liste ou convertible en liste de cha√Ænes de caract√®res.\")\n",
    "        \n",
    "        # Nettoyage et conversion en string\n",
    "        X_cleaned = [str(text) if not pd.isna(text) else \"\" for text in X]\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        for text in tqdm(X_cleaned, desc=f\"Embeddings {self.model_name}\"):\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                                    padding=True, max_length=self.max_length)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            \n",
    "            if self.embedding_strategy == 'cls':\n",
    "                embedding = last_hidden_states[:, 0, :].squeeze().numpy()\n",
    "            else:  # mean\n",
    "                embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "def test_multiple_classifiers(model_name, twenty_train, twenty_test, embedding_strategy=\"cls\"):\n",
    "    \"\"\"\n",
    "    Cr√©e des pipelines sklearn avec BERTEmbedder et diff√©rents classifieurs,\n",
    "    les entra√Æne et les √©value\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Nom du mod√®le BERT √† utiliser\n",
    "        twenty_train: Donn√©es d'entra√Ænement\n",
    "        twenty_test: Donn√©es de test\n",
    "        embedding_strategy (str): Strat√©gie d'embedding (\"cls\" ou \"mean\")\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (meilleur_classifieur, scores)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Test du mod√®le {model_name} (embedding par: {embedding_strategy}) ===\")\n",
    "    \n",
    "    # Assurer que processed_text est une liste de strings\n",
    "    X_train = twenty_train.processed_text.tolist() if hasattr(twenty_train, 'processed_text') else twenty_train.tolist()\n",
    "    X_test = twenty_test.processed_text.tolist() if hasattr(twenty_test, 'processed_text') else twenty_test.tolist()\n",
    "    \n",
    "    y_train = twenty_train.target if hasattr(twenty_train, 'target') else twenty_train\n",
    "    y_test = twenty_test.target if hasattr(twenty_test, 'target') else twenty_test\n",
    "    \n",
    "    # Cr√©er l'embedder BERT\n",
    "    embedder = BERTEmbedder(model_name=model_name, max_length=128, embedding_strategy=embedding_strategy)\n",
    "    \n",
    "    # Transformer les donn√©es une seule fois pour √©viter de r√©p√©ter cette op√©ration co√ªteuse\n",
    "    print(\"Cr√©ation des embeddings (peut prendre du temps)...\")\n",
    "    X_train_embedded = embedder.fit_transform(X_train)\n",
    "    X_test_embedded = embedder.transform(X_test)\n",
    "    \n",
    "    # D√©finir les classifieurs √† tester\n",
    "    classifiers = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "        'SVM': SVC(class_weight='balanced', random_state=42, probability=True),\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'LightGBM': lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Tester chaque classifieur\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nEntra√Ænement du mod√®le {name}...\")\n",
    "        clf.fit(X_train_embedded, y_train)\n",
    "        y_pred = clf.predict(X_test_embedded)\n",
    "        \n",
    "        # √âvaluer les performances avec focus sur la classe 1\n",
    "        accuracy = (y_pred == y_test).mean()\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall_class1 = recall_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0\n",
    "        f1_class1 = f1_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'recall_class1': recall_class1,\n",
    "            'f1_class1': f1_class1,\n",
    "            'clf': clf\n",
    "        }\n",
    "        \n",
    "        print(f\"R√©sultats pour {name}:\")\n",
    "        print(f\"Exactitude: {accuracy:.4f}\")\n",
    "        print(f\"Recall (g√©n√©ral): {recall:.4f}\")\n",
    "        print(f\"F1-score (g√©n√©ral): {f1:.4f}\")\n",
    "        print(f\"Recall (classe 1): {recall_class1:.4f}\")\n",
    "        print(f\"F1-score (classe 1): {f1_class1:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # D√©terminer le meilleur classifieur en fonction du F1-score et recall pour classe 1\n",
    "    best_score = 0\n",
    "    best_classifier = None\n",
    "    \n",
    "    for name, scores in results.items():\n",
    "        # Score combin√© donnant un poids √©gal au recall et F1 pour la classe 1\n",
    "        score = (scores['f1_class1'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_classifier = name\n",
    "    \n",
    "    print(f\"\\nMeilleur classifieur: {best_classifier} avec un score de {best_score:.4f}\")\n",
    "    \n",
    "    # Cr√©er une pipeline compl√®te avec le meilleur classifieur\n",
    "    best_pipeline = Pipeline([\n",
    "        ('embedder', embedder),\n",
    "        ('classifier', results[best_classifier]['clf'])\n",
    "    ])\n",
    "    \n",
    "    return best_classifier, results, best_pipeline, embedder\n",
    "\n",
    "def optimize_best_classifier(best_classifier, X_train_embedded, y_train, X_test_embedded, y_test):\n",
    "    \"\"\"\n",
    "    Optimise les hyperparam√®tres du meilleur classifieur\n",
    "    \n",
    "    Args:\n",
    "        best_classifier (str): Nom du meilleur classifieur\n",
    "        X_train_embedded: Donn√©es d'entra√Ænement embed√©es\n",
    "        y_train: Labels d'entra√Ænement\n",
    "        X_test_embedded: Donn√©es de test embed√©es\n",
    "        y_test: Labels de test\n",
    "        \n",
    "    Returns:\n",
    "        model: Mod√®le optimis√©\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Optimisation du classifieur {best_classifier} ===\")\n",
    "    \n",
    "    # D√©finir les grilles de param√®tres pour chaque classifieur\n",
    "    param_grids = {\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear', 'lbfgs'],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # S√©lectionner le classifieur et la grille de param√®tres appropri√©s\n",
    "    if best_classifier == 'LogisticRegression':\n",
    "        clf = LogisticRegression(class_weight='balanced', random_state=42, max_iter=2000)\n",
    "    elif best_classifier == 'SVM':\n",
    "        clf = SVC(class_weight='balanced', random_state=42, probability=True)\n",
    "    elif best_classifier == 'XGBoost':\n",
    "        clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    elif best_classifier == 'LightGBM':\n",
    "        clf = lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "    else:\n",
    "        raise ValueError(f\"Classifieur {best_classifier} non reconnu\")\n",
    "    \n",
    "    param_grid = param_grids[best_classifier]\n",
    "    \n",
    "    # Optimisation par validation crois√©e\n",
    "    grid_search = GridSearchCV(\n",
    "        clf, param_grid,\n",
    "        cv=3,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_embedded, y_train)\n",
    "    \n",
    "    # R√©cup√©rer le meilleur mod√®le\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Meilleurs param√®tres: {grid_search.best_params_}\")\n",
    "    \n",
    "    # √âvaluer le mod√®le optimis√©\n",
    "    y_pred = best_model.predict(X_test_embedded)\n",
    "    \n",
    "    print(\"\\nR√©sultats apr√®s optimisation:\")\n",
    "    print(f\"Exactitude: {(y_pred == y_test).mean():.4f}\")\n",
    "    print(f\"Recall (classe 1): {recall_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0:.4f}\")\n",
    "    print(f\"F1-score (classe 1): {f1_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def run_bert_experiments(twenty_train, twenty_test):\n",
    "    \"\"\"\n",
    "    Ex√©cute des exp√©riences avec diff√©rents mod√®les BERT, strat√©gies d'embedding\n",
    "    et diff√©rents classifieurs\n",
    "    \n",
    "    Args:\n",
    "        twenty_train: Donn√©es d'entra√Ænement\n",
    "        twenty_test: Donn√©es de test\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (meilleur_mod√®le, meilleur_embedder)\n",
    "    \"\"\"\n",
    "    models = [\n",
    "        'bert-base-multilingual-cased',  # BERT multilingue (pour plusieurs langues)\n",
    "        'distilbert-base-uncased',       # DistilBERT (version plus l√©g√®re)\n",
    "    ]\n",
    "    \n",
    "    strategies = ['mean', 'cls']  # Strat√©gies d'embedding: 'mean' ou 'cls'\n",
    "    \n",
    "    results = {}\n",
    "    best_pipelines = {}\n",
    "    \n",
    "    for model in models:\n",
    "        for strategy in strategies:\n",
    "            model_key = f\"{model}_{strategy}\"\n",
    "            best_clf, clf_results, pipeline, embedder = test_multiple_classifiers(\n",
    "                model, twenty_train, twenty_test, embedding_strategy=strategy\n",
    "            )\n",
    "            \n",
    "            # Stocker les r√©sultats\n",
    "            results[model_key] = {\n",
    "                'best_classifier': best_clf,\n",
    "                'classifier_results': clf_results\n",
    "            }\n",
    "            \n",
    "            best_pipelines[model_key] = pipeline\n",
    "    \n",
    "    # D√©terminer la meilleure combinaison (mod√®le BERT + strat√©gie + classifieur)\n",
    "    best_score = 0\n",
    "    best_config = None\n",
    "    \n",
    "    print(\"\\n=== R√©sum√© des r√©sultats ===\")\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        model, strategy = model_key.split('_')\n",
    "        best_clf = result['best_classifier']\n",
    "        clf_scores = result['classifier_results'][best_clf]\n",
    "        \n",
    "        # Score combin√© (recall + f1 pour classe 1)\n",
    "        score = (clf_scores['f1_class1'])\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Mod√®le BERT': model,\n",
    "            'Strat√©gie': strategy,\n",
    "            'Classifieur': best_clf,\n",
    "            'Recall (classe 1)': clf_scores['recall_class1'],\n",
    "            'F1 (classe 1)': clf_scores['f1_class1'],\n",
    "            'Score': score\n",
    "        })\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_config = model_key\n",
    "    \n",
    "    # Afficher un r√©sum√© des r√©sultats\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.sort_values('Score', ascending=False))\n",
    "    \n",
    "    if best_config:\n",
    "        print(f\"\\nMeilleure configuration: {best_config} avec {results[best_config]['best_classifier']}\")\n",
    "        best_model_key = best_config\n",
    "        best_clf_name = results[best_config]['best_classifier']\n",
    "        \n",
    "        # Pr√©parer les donn√©es pour l'optimisation\n",
    "        model_name, embedding_strategy = best_model_key.split('_')\n",
    "        embedder = BERTEmbedder(model_name=model_name, max_length=128, embedding_strategy=embedding_strategy)\n",
    "        \n",
    "        # Transformer les donn√©es\n",
    "        X_train = twenty_train.processed_text.tolist() if hasattr(twenty_train, 'processed_text') else twenty_train.tolist()\n",
    "        X_test = twenty_test.processed_text.tolist() if hasattr(twenty_test, 'processed_text') else twenty_test.tolist()\n",
    "        y_train = twenty_train.target if hasattr(twenty_train, 'target') else twenty_train\n",
    "        y_test = twenty_test.target if hasattr(twenty_test, 'target') else twenty_test\n",
    "        \n",
    "        print(\"\\nCr√©ation des embeddings pour l'optimisation...\")\n",
    "        X_train_embedded = embedder.fit_transform(X_train)\n",
    "        X_test_embedded = embedder.transform(X_test)\n",
    "        \n",
    "        # Optimiser le meilleur classifieur\n",
    "        best_model = optimize_best_classifier(\n",
    "            best_clf_name, X_train_embedded, y_train, X_test_embedded, y_test\n",
    "        )\n",
    "        \n",
    "        # Cr√©er la pipeline finale\n",
    "        final_pipeline = Pipeline([\n",
    "            ('embedder', embedder),\n",
    "            ('classifier', best_model)\n",
    "        ])\n",
    "\n",
    "        # Exporter le mod√®le\n",
    "        print(\"\\nExportation du mod√®le au format pkl...\")\n",
    "        with open(f'bert_model.pkl', 'wb') as f:\n",
    "            pickle.dump(final_pipeline, f)\n",
    "        print(f\"Mod√®le export√©: bert_model.pkl\")\n",
    "        \n",
    "        return final_pipeline, embedder\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Pour ex√©cuter les exp√©riences:\n",
    "final_pipeline, embedder = run_bert_experiments(X_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
