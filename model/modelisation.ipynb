{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b9eb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "\n",
    "# Sauvegarde des modèles\n",
    "import joblib\n",
    "\n",
    "# Système et utilitaires\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# API et web\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Implicite \n",
    "import streamlit\n",
    "import requests\n",
    "import uvicorn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f0f18",
   "metadata": {},
   "source": [
    "# 5. VECTORISATION ET MODÉLISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02822ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "combined_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cleaned_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "processed_text",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3fda8ef7-e6e6-4895-adaa-158c1422c344",
       "rows": [
        [
         "0",
         "0",
         "ablaze",
         null,
         "Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…",
         "1",
         "Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze… ablaze",
         "communal violenc bhainsa telangana stone pelt muslim hous hous vehicl set ablaz ablaz",
         "125",
         "communal violenc in bhainsa telangana stone were pelt on muslim hous and some hous and vehicl were set ablaz"
        ],
        [
         "1",
         "1",
         "ablaze",
         null,
         "Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po…",
         "1",
         "Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po… ablaze",
         "telangana section impos bhainsa januari clash erupt two group januari po ablaz",
         "131",
         "telangana section has been impos in bhainsa from januari to after clash erupt between two group on januari po"
        ],
        [
         "2",
         "2",
         "ablaze",
         "New York City",
         "Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI",
         "1",
         "Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI ablaze",
         "arsonist set car ablaz dealership ablaz",
         "63",
         "arsonist set car ablaz at dealership"
        ],
        [
         "3",
         "4",
         "ablaze",
         null,
         "\"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi8",
         "0",
         "\"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi8 ablaze",
         "lord jesus love bring freedom pardon fill holi spirit set heart ablaz l ablaz",
         "140",
         "lord jesus your love bring freedom and pardon fill me with your holi spirit and set my heart ablaz with your l"
        ],
        [
         "4",
         "5",
         "ablaze",
         "OC",
         "If this child was Chinese, this tweet would have gone viral. Social media would be ablaze. SNL would have made a racist j…",
         "0",
         "If this child was Chinese, this tweet would have gone viral. Social media would be ablaze. SNL would have made a racist j… ablaze",
         "child chines tweet would gone viral social media would ablaz snl would made racist j ablaz",
         "122",
         "if this child was chines this tweet would have gone viral social media would be ablaz snl would have made a racist j"
        ],
        [
         "5",
         "6",
         "ablaze",
         "London, England",
         "Several houses have been set ablaze in Ngemsibaa village, Oku sub division in the North West Region of Cameroon by… https://t.co/99uHGAzxy2",
         "1",
         "Several houses have been set ablaze in Ngemsibaa village, Oku sub division in the North West Region of Cameroon by… https://t.co/99uHGAzxy2 ablaze",
         "sever hous set ablaz ngemsibaa villag oku sub divis north west region cameroon ablaz",
         "139",
         "sever hous have been set ablaz in ngemsibaa villag oku sub divis in the north west region of cameroon by"
        ],
        [
         "6",
         "7",
         "ablaze",
         "Bharat",
         "Asansol: A BJP office in Salanpur village was set ablaze last night. BJP has alleged that TMC is behind the incident. Police has b…",
         "1",
         "Asansol: A BJP office in Salanpur village was set ablaze last night. BJP has alleged that TMC is behind the incident. Police has b… ablaze",
         "asansol bjp offic salanpur villag set ablaz last night bjp alleg tmc behind incid polic b ablaz",
         "131",
         "asansol a bjp offic in salanpur villag was set ablaz last night bjp has alleg that tmc is behind the incid polic has b"
        ],
        [
         "7",
         "8",
         "ablaze",
         "Accra, Ghana",
         "National Security Minister, Kan Dapaah's side chic has set the internet ablaze with her latest powerful video.… https://t.co/rhzOMQVSlj",
         "0",
         "National Security Minister, Kan Dapaah's side chic has set the internet ablaze with her latest powerful video.… https://t.co/rhzOMQVSlj ablaze",
         "nation secur minist kan dapaah side chic set internet ablaz latest power video ablaz",
         "135",
         "nation secur minist kan dapaah s side chic has set the internet ablaz with her latest power video"
        ],
        [
         "8",
         "9",
         "ablaze",
         "Searching",
         "This creature who’s soul is no longer clarent but blue ablaze This thing Carrying memories Memories of… https://t.co/tBKSNDrDoX",
         "0",
         "This creature who’s soul is no longer clarent but blue ablaze This thing Carrying memories Memories of… https://t.co/tBKSNDrDoX ablaze",
         "creatur soul longer clarent blue ablaz thing carri memori memori ablaz",
         "127",
         "this creatur who is soul is no longer clarent but blue ablaz this thing carri memori memori of"
        ],
        [
         "9",
         "10",
         "ablaze",
         null,
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is reported…",
         "1",
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is reported… ablaze",
         "imag show havoc caus cameroon militari torch hous oku shameless militari report ablaz",
         "124",
         "imag show the havoc caus by the cameroon militari as they torch hous in oku the shameless militari is report"
        ],
        [
         "10",
         "11",
         "ablaze",
         null,
         "Social media went bananas after Chuba Hubbard announced Monday evening his plans to return to #okstate. https://t.co/peN…",
         "0",
         "Social media went bananas after Chuba Hubbard announced Monday evening his plans to return to #okstate. https://t.co/peN… ablaze",
         "social media went banana chuba hubbard announc monday even plan return okstat ablaz",
         "121",
         "social media went banana after chuba hubbard announc monday even his plan to return to okstat"
        ],
        [
         "11",
         "12",
         "ablaze",
         null,
         "Hausa youths set Area Office of Apapa-Iganmu Local Council Development Area ablaze. Okada Riders stormed the LG area office…",
         "1",
         "Hausa youths set Area Office of Apapa-Iganmu Local Council Development Area ablaze. Okada Riders stormed the LG area office… ablaze",
         "hausa youth set area offic apapa iganmu local council develop area ablaz okada rider storm lg area offic ablaz",
         "124",
         "hausa youth set area offic of apapa iganmu local council develop area ablaz okada rider storm the lg area offic"
        ],
        [
         "12",
         "13",
         "ablaze",
         "HYDERABAD",
         "Under #MamataBanerjee political violence &amp; vandalism continues to unabated in West Bengal! office in Asanol was…",
         "1",
         "Under #MamataBanerjee political violence &amp; vandalism continues to unabated in West Bengal! office in Asanol was… ablaze",
         "mamatabanerje polit violenc amp vandal continu unab west bengal offic asanol ablaz",
         "116",
         "under mamatabanerje polit violenc amp vandal continu to unab in west bengal offic in asanol was"
        ],
        [
         "13",
         "14",
         "ablaze",
         "Reno, NV",
         "AMEN! Set the whole system ablaze, man. https://t.co/J08xHDcGbD",
         "0",
         "AMEN! Set the whole system ablaze, man. https://t.co/J08xHDcGbD ablaze",
         "amen set whole system ablaz man ablaz",
         "63",
         "amen set the whole system ablaz man"
        ],
        [
         "14",
         "15",
         "ablaze",
         null,
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is… https://t.co/gIwZCH533D",
         "1",
         "Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is… https://t.co/gIwZCH533D ablaze",
         "imag show havoc caus cameroon militari torch hous oku shameless militari ablaz",
         "139",
         "imag show the havoc caus by the cameroon militari as they torch hous in oku the shameless militari is"
        ],
        [
         "15",
         "16",
         "ablaze",
         null,
         "No cows today but our local factory is sadly still ablaze #REDJanuary2020 https://t.co/CMyuKzrcKz",
         "1",
         "No cows today but our local factory is sadly still ablaze #REDJanuary2020 https://t.co/CMyuKzrcKz ablaze",
         "cow today local factori sad still ablaz redjanuari ablaz",
         "97",
         "no cow today but our local factori is sad still ablaz redjanuari"
        ],
        [
         "16",
         "17",
         "ablaze",
         null,
         "Rengoku sets my heart ablaze😔❤️🔥 P.s. I missed this style of coloring I do so here it is c: #鬼滅の刃 https://t.co/YrUF9g68s0",
         "0",
         "Rengoku sets my heart ablaze😔❤️🔥 P.s. I missed this style of coloring I do so here it is c: #鬼滅の刃 https://t.co/YrUF9g68s0 ablaze",
         "rengoku set heart ablaz p miss style color c 鬼滅の刃 ablaz",
         "121",
         "rengoku set my heart ablaz p s i miss this style of color i do so here it is c 鬼滅の刃"
        ],
        [
         "17",
         "18",
         "ablaze",
         "Worldwide",
         "paulzizkaphoto: “Rundle Ablaze” Wishing you all a good evening... https://t.co/d0NlME1HQz https://t.co/hlVlT6qiIp",
         "0",
         "paulzizkaphoto: “Rundle Ablaze” Wishing you all a good evening... https://t.co/d0NlME1HQz https://t.co/hlVlT6qiIp ablaze",
         "paulzizkaphoto rundl ablaz wish good even ablaz",
         "113",
         "paulzizkaphoto rundl ablaz wish you all a good even"
        ],
        [
         "18",
         "19",
         "ablaze",
         null,
         "French cameroun set houses ablaze in Ndu and roasted two young boys in their homes in #targeted killings in a #GenocideInSou…",
         "1",
         "French cameroun set houses ablaze in Ndu and roasted two young boys in their homes in #targeted killings in a #GenocideInSou… ablaze",
         "french cameroun set hous ablaz ndu roast two young boy home target kill genocideinsou ablaz",
         "125",
         "french cameroun set hous ablaz in ndu and roast two young boy in their home in target kill in a genocideinsou"
        ],
        [
         "19",
         "20",
         "ablaze",
         null,
         "Cameroon's #BIR soldiers on the 05/01/2020 invaded the #SouthernCameroons Village of Kimar - So setting ablaze a total of…",
         "1",
         "Cameroon's #BIR soldiers on the 05/01/2020 invaded the #SouthernCameroons Village of Kimar - So setting ablaze a total of… ablaze",
         "cameroon bir soldier invad southerncameroon villag kimar set ablaz total ablaz",
         "122",
         "cameroon s bir soldier on the invad the southerncameroon villag of kimar so set ablaz a total of"
        ],
        [
         "20",
         "21",
         "ablaze",
         null,
         "As fires ablaze throughout the land/as the prophosized apocalypse comes to fruition/not a building standing/as we all come st…",
         "1",
         "As fires ablaze throughout the land/as the prophosized apocalypse comes to fruition/not a building standing/as we all come st… ablaze",
         "fire ablaz throughout land prophos apocalyps come fruition build stand come st ablaz",
         "126",
         "as fire ablaz throughout the land as the prophos apocalyps come to fruition not a build stand as we all come st"
        ],
        [
         "21",
         "22",
         "ablaze",
         "Italy",
         "#ThankfulTuesday Isaiah 43:2 When you pass through the waters, I will be with you; and when you pass through the r… https://t.co/jEp3L1oU36",
         "0",
         "#ThankfulTuesday Isaiah 43:2 When you pass through the waters, I will be with you; and when you pass through the r… https://t.co/jEp3L1oU36 ablaze",
         "thankfultuesday isaiah pass water pass r ablaz",
         "139",
         "thankfultuesday isaiah when you pass through the water i will be with you and when you pass through the r"
        ],
        [
         "22",
         "23",
         "ablaze",
         "` ˗ˏˋ i'm⠀waiting⠀for⠀you⠀to⠀pour⠀my⠀𝘀𝗶𝗻𝘀⠀onto⠀me⠀;⠀call⠀me⠀your⠀𝘭𝘰𝘷𝘦𝘳⠀whispering⠀hymns⠀to⠀my⠀ear⠀,⠀crucify⠀me⠀in⠀claims⠀of⠀praising⠀me .",
         "⠀⠀When you walk through the fire, ⠀⠀you will not be scorched, and the ⠀⠀flames will not set you ablaze. ──── product…",
         "0",
         "⠀⠀When you walk through the fire, ⠀⠀you will not be scorched, and the ⠀⠀flames will not set you ablaze. ──── product… ablaze",
         "walk fire scorch flame set ablaz product ablaz",
         "117",
         "when you walk through the fire you will not be scorch and the flame will not set you ablaz product"
        ],
        [
         "23",
         "24",
         "ablaze",
         "Okielahoma",
         "Originally they were intended to be fired at boarders or the opposing ship's crew from gunners in the ships's riggi… https://t.co/z5vPwuyBCL",
         "1",
         "Originally they were intended to be fired at boarders or the opposing ship's crew from gunners in the ships's riggi… https://t.co/z5vPwuyBCL ablaze",
         "origin intend fire boarder oppos ship crew gunner ship riggi ablaz",
         "140",
         "origin they were intend to be fire at boarder or the oppos ship s crew from gunner in the ship s riggi"
        ],
        [
         "24",
         "25",
         "ablaze",
         null,
         "Warm greetings to all on the occasion of #Lohri. As winter passes by may everyone's woes and troubles be set ablaze in t…",
         "0",
         "Warm greetings to all on the occasion of #Lohri. As winter passes by may everyone's woes and troubles be set ablaze in t… ablaze",
         "warm greet occas lohri winter pass may everyon woe troubl set ablaz ablaz",
         "121",
         "warm greet to all on the occas of lohri as winter pass by may everyon is woe and troubl be set ablaz in t"
        ],
        [
         "25",
         "26",
         "ablaze",
         null,
         "Another arson in Njikom,Boyo,NWR. The ambazombies yesterday 13/1/2020 set ablaze the Council building. Which country in the wo…",
         "1",
         "Another arson in Njikom,Boyo,NWR. The ambazombies yesterday 13/1/2020 set ablaze the Council building. Which country in the wo… ablaze",
         "anoth arson njikom boyo nwr ambazombi yesterday set ablaz council build countri wo ablaz",
         "127",
         "anoth arson in njikom boyo nwr the ambazombi yesterday set ablaz the council build which countri in the wo"
        ],
        [
         "26",
         "27",
         "ablaze",
         "Havana 3am",
         "Another public market in #Haiti mysteriously set ablaze. The independent merchants, who are overwhelmingly women, become…",
         "1",
         "Another public market in #Haiti mysteriously set ablaze. The independent merchants, who are overwhelmingly women, become… ablaze",
         "anoth public market haiti mysteri set ablaz independ merchant overwhelm women becom ablaz",
         "121",
         "anoth public market in haiti mysteri set ablaz the independ merchant who are overwhelm women becom"
        ],
        [
         "27",
         "28",
         "ablaze",
         null,
         "that is kind true sadly",
         "0",
         "that is kind true sadly ablaze",
         "kind true sad ablaz",
         "23",
         "that is kind true sad"
        ],
        [
         "28",
         "29",
         "ablaze",
         null,
         "I swear that jam will set the world ablaze",
         "0",
         "I swear that jam will set the world ablaze ablaze",
         "swear jam set world ablaz ablaz",
         "42",
         "i swear that jam will set the world ablaz"
        ],
        [
         "29",
         "30",
         "ablaze",
         null,
         "Marivan, Kurdistan Province Monday, Jan 13th, 2020 Protesters set the propaganda banner of #QassemSoleimani ablaze. #I…",
         "1",
         "Marivan, Kurdistan Province Monday, Jan 13th, 2020 Protesters set the propaganda banner of #QassemSoleimani ablaze. #I… ablaze",
         "marivan kurdistan provinc monday jan th protest set propaganda banner qassemsoleimani ablaz ablaz",
         "119",
         "marivan kurdistan provinc monday jan th protest set the propaganda banner of qassemsoleimani ablaz i"
        ],
        [
         "30",
         "32",
         "ablaze",
         "India",
         "How can you turn a blind eye to the icident of setting ablaze more than 50 houses of H… https://t.co/UD84oQxg9K",
         "1",
         "How can you turn a blind eye to the icident of setting ablaze more than 50 houses of H… https://t.co/UD84oQxg9K ablaze",
         "turn blind eye icid set ablaz hous h ablaz",
         "111",
         "how can you turn a blind eye to the icid of set ablaz more than hous of h"
        ],
        [
         "31",
         "33",
         "ablaze",
         "Salta, Argentina",
         "This love is so completely crazy. You've been fucking with my dreams, ripped me like your torn up jeans. I don't ev… https://t.co/GCVYW5eZkb",
         "0",
         "This love is so completely crazy. You've been fucking with my dreams, ripped me like your torn up jeans. I don't ev… https://t.co/GCVYW5eZkb ablaze",
         "love complet crazi fuck dream rip like torn jean ev ablaz",
         "140",
         "this love is so complet crazi you have been fuck with my dream rip me like your torn up jean i do not ev"
        ],
        [
         "32",
         "34",
         "accident",
         "Wherever socks go in the dryer",
         "Terms in A Demon Burning Dark: The Ruined: People who cannot use magic or interact with it without some harm or ac… https://t.co/ZEDawOfuu4",
         "0",
         "Terms in A Demon Burning Dark: The Ruined: People who cannot use magic or interact with it without some harm or ac… https://t.co/ZEDawOfuu4 accident",
         "term demon burn dark ruin peopl use magic interact without harm ac accid",
         "139",
         "term in a demon burn dark the ruin peopl who can not use magic or interact with it without some harm or ac"
        ],
        [
         "33",
         "35",
         "accident",
         "Kuala Lumpur",
         "📷 Heartfelt appreciation to Prime Minister YAB Tun Dr. wife, YABhg. Tun Dr. Siti Hasmah Mohd Ali fo… https://t.co/YOwUp1BYUP",
         "0",
         "📷 Heartfelt appreciation to Prime Minister YAB Tun Dr. wife, YABhg. Tun Dr. Siti Hasmah Mohd Ali fo… https://t.co/YOwUp1BYUP accident",
         "heartfelt appreci prime minist yab tun dr wife yabhg tun dr siti hasmah mohd ali fo accid",
         "124",
         "heartfelt appreci to prime minist yab tun dr wife yabhg tun dr siti hasmah mohd ali fo"
        ],
        [
         "34",
         "36",
         "accident",
         null,
         "#WATCH Former CM Akhilesh Yadav who went to meet injured of Kannauj accident, at a hospital in Chhibramau asks Emergency Med…",
         "1",
         "#WATCH Former CM Akhilesh Yadav who went to meet injured of Kannauj accident, at a hospital in Chhibramau asks Emergency Med… accident",
         "watch former cm akhilesh yadav went meet injur kannauj accid hospit chhibramau ask emerg med accid",
         "125",
         "watch former cm akhilesh yadav who went to meet injur of kannauj accid at a hospit in chhibramau ask emerg med"
        ],
        [
         "35",
         "37",
         "accident",
         "Sydney, New South Wales",
         "❤️❤️❤️ he gave us everything... He had a horrible foot infection once so wore one thong… https://t.co/mA9sFl6Shw",
         "0",
         "❤️❤️❤️ he gave us everything... He had a horrible foot infection once so wore one thong… https://t.co/mA9sFl6Shw accident",
         "gave us everyth horribl foot infect wore one thong accid",
         "112",
         "he gave us everyth he had a horribl foot infect onc so wore one thong"
        ],
        [
         "36",
         "38",
         "accident",
         "Nigeria",
         "😁yeah! His new swag is on point 100%, since the accident! Like this is a totally transformed Bob…",
         "0",
         "😁yeah! His new swag is on point 100%, since the accident! Like this is a totally transformed Bob… accident",
         "yeah new swag point sinc accid like total transform bob accid",
         "97",
         "yeah his new swag is on point sinc the accid like this is a total transform bob"
        ],
        [
         "37",
         "39",
         "accident",
         "Davanagere, India ",
         "This is cool and all these days I have been doing \"git push origin CURRENT_BRANCH_NAME\". You know that… https://t.co/mr0YAGEWqj",
         "0",
         "This is cool and all these days I have been doing \"git push origin CURRENT_BRANCH_NAME\". You know that… https://t.co/mr0YAGEWqj accident",
         "cool day git push origin current_branch_nam know accid",
         "127",
         "this is cool and all these day i have been do git push origin current_branch_nam you know that"
        ],
        [
         "38",
         "40",
         "accident",
         "Lyndhurst, OH",
         "#Preorder #newrelease today! 12 witnesses connected to or investigating #THENUTCRACKERCONSPIRACY have died either in a…",
         "1",
         "#Preorder #newrelease today! 12 witnesses connected to or investigating #THENUTCRACKERCONSPIRACY have died either in a… accident",
         "preorder newreleas today wit connect investig thenutcrackerconspiraci die either accid",
         "119",
         "preorder newreleas today wit connect to or investig thenutcrackerconspiraci have die either in a"
        ],
        [
         "39",
         "41",
         "accident",
         "Covina, Ca",
         "my back and neck are still fucked up from the accident 😡😡😭😭",
         "0",
         "my back and neck are still fucked up from the accident 😡😡😭😭 accident",
         "back neck still fuck accid accid",
         "59",
         "my back and neck are still fuck up from the accid"
        ],
        [
         "40",
         "42",
         "accident",
         "Colorado, Brighton",
         "RT! Prince Harry just confirmed that his mother’s (Princess Diana) death was not an accident! https://t.co/1ADe3uZ3eR",
         "1",
         "RT! Prince Harry just confirmed that his mother’s (Princess Diana) death was not an accident! https://t.co/1ADe3uZ3eR accident",
         "rt princ harri confirm mother princess diana death accid accid",
         "117",
         "rt princ harri just confirm that his mother s princess diana death was not an accid"
        ],
        [
         "41",
         "43",
         "accident",
         null,
         "Note to Democrats: It’s not a Muslim ban. Islam is not a race. Soleimani was a terrorist &amp; was exterminated not assas…",
         "0",
         "Note to Democrats: It’s not a Muslim ban. Islam is not a race. Soleimani was a terrorist &amp; was exterminated not assas… accident",
         "note democrat muslim ban islam race soleimani terrorist amp extermin assa accid",
         "122",
         "note to democrat it is not a muslim ban islam is not a race soleimani was a terrorist amp was extermin not assa"
        ],
        [
         "42",
         "44",
         "accident",
         "North Carolina, USA",
         "Juwan Johnson/Oregon is one big dude. Looks like a tight end stuck in the receiver group by accident.",
         "0",
         "Juwan Johnson/Oregon is one big dude. Looks like a tight end stuck in the receiver group by accident. accident",
         "juwan johnson oregon one big dude look like tight end stuck receiv group accid accid",
         "101",
         "juwan johnson oregon is one big dude look like a tight end stuck in the receiv group by accid"
        ],
        [
         "43",
         "45",
         "accident",
         "kurger bing",
         "More appearances of the man with the upside-down face. A New Year’s Eve party at an Air Force base in 1943 where a man…",
         "0",
         "More appearances of the man with the upside-down face. A New Year’s Eve party at an Air Force base in 1943 where a man… accident",
         "appear man upsid face new year eve parti air forc base man accid",
         "119",
         "more appear of the man with the upsid down face a new year s eve parti at an air forc base in where a man"
        ],
        [
         "44",
         "46",
         "accident",
         "Faridabad, India",
         "The speeding car rammed into a group of people, who were returning after attending a temple festival of Ayyappan Kavu in Thum…",
         "1",
         "The speeding car rammed into a group of people, who were returning after attending a temple festival of Ayyappan Kavu in Thum… accident",
         "speed car ram group peopl return attend templ festiv ayyappan kavu thum accid",
         "126",
         "the speed car ram into a group of peopl who were return after attend a templ festiv of ayyappan kavu in thum"
        ],
        [
         "45",
         "47",
         "accident",
         "60",
         "My friend (an army) just lost her father in an accident and her mom right now is still unconscious. Please pray for her mo…",
         "1",
         "My friend (an army) just lost her father in an accident and her mom right now is still unconscious. Please pray for her mo… accident",
         "friend armi lost father accid mom right still unconsci pleas pray mo accid",
         "123",
         "my friend an armi just lost her father in an accid and her mom right now is still unconsci pleas pray for her mo"
        ],
        [
         "46",
         "48",
         "accident",
         "ngovhela Mahunguni",
         "MLINDO THE VOCALIST IN ANOTHER CAR ACCIDENT https://t.co/BXR9rEgAk6",
         "1",
         "MLINDO THE VOCALIST IN ANOTHER CAR ACCIDENT https://t.co/BXR9rEgAk6 accident",
         "mlindo vocalist anoth car accid accid",
         "67",
         "mlindo the vocalist in anoth car accid"
        ],
        [
         "47",
         "49",
         "accident",
         null,
         "“There are no greater treasures than the highest human qualities such as compassion, courage and hope. Not even tragic accide…",
         "0",
         "“There are no greater treasures than the highest human qualities such as compassion, courage and hope. Not even tragic accide… accident",
         "greater treasur highest human qualiti compass courag hope even tragic accid accid",
         "126",
         "there are no greater treasur than the highest human qualiti such as compass courag and hope not even tragic accid"
        ],
        [
         "48",
         "50",
         "accident",
         "Germany",
         "Please help our friends in - they have had a non fault accident that's resulted in their vehicle being writte…",
         "0",
         "Please help our friends in - they have had a non fault accident that's resulted in their vehicle being writte… accident",
         "pleas help friend non fault accid result vehicl writt accid",
         "110",
         "pleas help our friend in they have had a non fault accid that is result in their vehicl be writt"
        ],
        [
         "49",
         "51",
         "accident",
         "South Beach, FL",
         "When you hurt your younger sibling by “accident” https://t.co/DAMTEoQtZU",
         "0",
         "When you hurt your younger sibling by “accident” https://t.co/DAMTEoQtZU accident",
         "hurt younger sibl accid accid",
         "72",
         "when you hurt your younger sibl by accid"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10803
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>communal violenc bhainsa telangana stone pelt ...</td>\n",
       "      <td>125</td>\n",
       "      <td>communal violenc in bhainsa telangana stone we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>telangana section impos bhainsa januari clash ...</td>\n",
       "      <td>131</td>\n",
       "      <td>telangana section has been impos in bhainsa fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>arsonist set car ablaz dealership ablaz</td>\n",
       "      <td>63</td>\n",
       "      <td>arsonist set car ablaz at dealership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>lord jesus love bring freedom pardon fill holi...</td>\n",
       "      <td>140</td>\n",
       "      <td>lord jesus your love bring freedom and pardon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>If this child was Chinese, this tweet would ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>If this child was Chinese, this tweet would ha...</td>\n",
       "      <td>child chines tweet would gone viral social med...</td>\n",
       "      <td>122</td>\n",
       "      <td>if this child was chines this tweet would have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10798</th>\n",
       "      <td>11364</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Had these guys last game n fcked them. Talked ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Had these guys last game n fcked them. Talked ...</td>\n",
       "      <td>guy last game n fcked talk non stop shit n sti...</td>\n",
       "      <td>139</td>\n",
       "      <td>had these guy last game n fcked them talk non ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>11365</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Blue State in a red sea</td>\n",
       "      <td>Media should have warned us well in advance. T...</td>\n",
       "      <td>0</td>\n",
       "      <td>Media should have warned us well in advance. T...</td>\n",
       "      <td>media warn us well advanc wreck whole night re...</td>\n",
       "      <td>92</td>\n",
       "      <td>media should have warn us well in advanc this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>11366</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>arohaonces</td>\n",
       "      <td>i feel directly attacked 💀 i consider moonbin ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i feel directly attacked 💀 i consider moonbin ...</td>\n",
       "      <td>feel direct attack consid moonbin amp jinjin b...</td>\n",
       "      <td>115</td>\n",
       "      <td>i feel direct attack i consid moonbin amp jinj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10801</th>\n",
       "      <td>11368</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>auroraborealis</td>\n",
       "      <td>ok who remember \"outcast\" nd the \"dora\" au?? T...</td>\n",
       "      <td>0</td>\n",
       "      <td>ok who remember \"outcast\" nd the \"dora\" au?? T...</td>\n",
       "      <td>ok rememb outcast nd dora au au wreck nerv nd ...</td>\n",
       "      <td>105</td>\n",
       "      <td>ok who rememb outcast nd the dora au those au ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>11369</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jake Corway wrecked while running 14th at IRP.</td>\n",
       "      <td>1</td>\n",
       "      <td>Jake Corway wrecked while running 14th at IRP....</td>\n",
       "      <td>jake corway wreck run th irp wreck</td>\n",
       "      <td>46</td>\n",
       "      <td>jake corway wreck while run th at irp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10803 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  keyword                 location  \\\n",
       "0          0   ablaze                      NaN   \n",
       "1          1   ablaze                      NaN   \n",
       "2          2   ablaze            New York City   \n",
       "3          4   ablaze                      NaN   \n",
       "4          5   ablaze                       OC   \n",
       "...      ...      ...                      ...   \n",
       "10798  11364  wrecked                      NaN   \n",
       "10799  11365  wrecked  Blue State in a red sea   \n",
       "10800  11366  wrecked               arohaonces   \n",
       "10801  11368  wrecked           auroraborealis   \n",
       "10802  11369  wrecked                      NaN   \n",
       "\n",
       "                                                    text  target  \\\n",
       "0      Communal violence in Bhainsa, Telangana. \"Ston...       1   \n",
       "1      Telangana: Section 144 has been imposed in Bha...       1   \n",
       "2      Arsonist sets cars ablaze at dealership https:...       1   \n",
       "3      \"Lord Jesus, your love brings freedom and pard...       0   \n",
       "4      If this child was Chinese, this tweet would ha...       0   \n",
       "...                                                  ...     ...   \n",
       "10798  Had these guys last game n fcked them. Talked ...       0   \n",
       "10799  Media should have warned us well in advance. T...       0   \n",
       "10800  i feel directly attacked 💀 i consider moonbin ...       0   \n",
       "10801  ok who remember \"outcast\" nd the \"dora\" au?? T...       0   \n",
       "10802     Jake Corway wrecked while running 14th at IRP.       1   \n",
       "\n",
       "                                           combined_text  \\\n",
       "0      Communal violence in Bhainsa, Telangana. \"Ston...   \n",
       "1      Telangana: Section 144 has been imposed in Bha...   \n",
       "2      Arsonist sets cars ablaze at dealership https:...   \n",
       "3      \"Lord Jesus, your love brings freedom and pard...   \n",
       "4      If this child was Chinese, this tweet would ha...   \n",
       "...                                                  ...   \n",
       "10798  Had these guys last game n fcked them. Talked ...   \n",
       "10799  Media should have warned us well in advance. T...   \n",
       "10800  i feel directly attacked 💀 i consider moonbin ...   \n",
       "10801  ok who remember \"outcast\" nd the \"dora\" au?? T...   \n",
       "10802  Jake Corway wrecked while running 14th at IRP....   \n",
       "\n",
       "                                            cleaned_text  text_length  \\\n",
       "0      communal violenc bhainsa telangana stone pelt ...          125   \n",
       "1      telangana section impos bhainsa januari clash ...          131   \n",
       "2                arsonist set car ablaz dealership ablaz           63   \n",
       "3      lord jesus love bring freedom pardon fill holi...          140   \n",
       "4      child chines tweet would gone viral social med...          122   \n",
       "...                                                  ...          ...   \n",
       "10798  guy last game n fcked talk non stop shit n sti...          139   \n",
       "10799  media warn us well advanc wreck whole night re...           92   \n",
       "10800  feel direct attack consid moonbin amp jinjin b...          115   \n",
       "10801  ok rememb outcast nd dora au au wreck nerv nd ...          105   \n",
       "10802                 jake corway wreck run th irp wreck           46   \n",
       "\n",
       "                                          processed_text  \n",
       "0      communal violenc in bhainsa telangana stone we...  \n",
       "1      telangana section has been impos in bhainsa fr...  \n",
       "2                   arsonist set car ablaz at dealership  \n",
       "3      lord jesus your love bring freedom and pardon ...  \n",
       "4      if this child was chines this tweet would have...  \n",
       "...                                                  ...  \n",
       "10798  had these guy last game n fcked them talk non ...  \n",
       "10799  media should have warn us well in advanc this ...  \n",
       "10800  i feel direct attack i consid moonbin amp jinj...  \n",
       "10801  ok who rememb outcast nd the dora au those au ...  \n",
       "10802              jake corway wreck while run th at irp  \n",
       "\n",
       "[10803 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des données processed\n",
    "df_processed = pd.read_csv('data/processed/tweets_processed.csv')\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b1f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données en features et target\n",
    "X = df_processed['processed_text']\n",
    "y = df_processed['target']\n",
    "# Séparation des données en train et test\n",
    "X_train, X_test = train_test_split(df_processed, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def258c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test du modèle bert-base-multilingual-cased (embedding par: mean) ===\n",
      "Création des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings bert-base-multilingual-cased: 100%|██████████| 8642/8642 [13:31<00:00, 10.65it/s]\n",
      "Embeddings bert-base-multilingual-cased: 100%|██████████| 2161/2161 [02:52<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle LogisticRegression...\n",
      "Résultats pour LogisticRegression:\n",
      "Exactitude: 0.8112\n",
      "Recall (général): 0.8112\n",
      "F1-score (général): 0.8260\n",
      "Recall (classe 1): 0.7507\n",
      "F1-score (classe 1): 0.5837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.88      1780\n",
      "           1       0.48      0.75      0.58       381\n",
      "\n",
      "    accuracy                           0.81      2161\n",
      "   macro avg       0.71      0.79      0.73      2161\n",
      "weighted avg       0.86      0.81      0.83      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle SVM...\n",
      "Résultats pour SVM:\n",
      "Exactitude: 0.8316\n",
      "Recall (général): 0.8316\n",
      "F1-score (général): 0.8442\n",
      "Recall (classe 1): 0.7900\n",
      "F1-score (classe 1): 0.6232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89      1780\n",
      "           1       0.51      0.79      0.62       381\n",
      "\n",
      "    accuracy                           0.83      2161\n",
      "   macro avg       0.73      0.82      0.76      2161\n",
      "weighted avg       0.87      0.83      0.84      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:02:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour XGBoost:\n",
      "Exactitude: 0.8760\n",
      "Recall (général): 0.8760\n",
      "F1-score (général): 0.8615\n",
      "Recall (classe 1): 0.4304\n",
      "F1-score (classe 1): 0.5503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      1780\n",
      "           1       0.76      0.43      0.55       381\n",
      "\n",
      "    accuracy                           0.88      2161\n",
      "   macro avg       0.83      0.70      0.74      2161\n",
      "weighted avg       0.87      0.88      0.86      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour LightGBM:\n",
      "Exactitude: 0.8561\n",
      "Recall (général): 0.8561\n",
      "F1-score (général): 0.8562\n",
      "Recall (classe 1): 0.5932\n",
      "F1-score (classe 1): 0.5924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      1780\n",
      "           1       0.59      0.59      0.59       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.75      0.75      0.75      2161\n",
      "weighted avg       0.86      0.86      0.86      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: SVM avec un score de 0.6232\n",
      "\n",
      "=== Test du modèle bert-base-multilingual-cased (embedding par: cls) ===\n",
      "Création des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings bert-base-multilingual-cased: 100%|██████████| 8642/8642 [11:54<00:00, 12.09it/s]\n",
      "Embeddings bert-base-multilingual-cased: 100%|██████████| 2161/2161 [02:51<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle LogisticRegression...\n",
      "Résultats pour LogisticRegression:\n",
      "Exactitude: 0.7885\n",
      "Recall (général): 0.7885\n",
      "F1-score (général): 0.8057\n",
      "Recall (classe 1): 0.7008\n",
      "F1-score (classe 1): 0.5388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.86      1780\n",
      "           1       0.44      0.70      0.54       381\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.68      0.75      0.70      2161\n",
      "weighted avg       0.84      0.79      0.81      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle SVM...\n",
      "Résultats pour SVM:\n",
      "Exactitude: 0.7881\n",
      "Recall (général): 0.7881\n",
      "F1-score (général): 0.8061\n",
      "Recall (classe 1): 0.7244\n",
      "F1-score (classe 1): 0.5465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86      1780\n",
      "           1       0.44      0.72      0.55       381\n",
      "\n",
      "    accuracy                           0.79      2161\n",
      "   macro avg       0.69      0.76      0.70      2161\n",
      "weighted avg       0.84      0.79      0.81      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:19:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour XGBoost:\n",
      "Exactitude: 0.8616\n",
      "Recall (général): 0.8616\n",
      "F1-score (général): 0.8396\n",
      "Recall (classe 1): 0.3360\n",
      "F1-score (classe 1): 0.4613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92      1780\n",
      "           1       0.74      0.34      0.46       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.80      0.66      0.69      2161\n",
      "weighted avg       0.85      0.86      0.84      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour LightGBM:\n",
      "Exactitude: 0.8607\n",
      "Recall (général): 0.8607\n",
      "F1-score (général): 0.8555\n",
      "Recall (classe 1): 0.5197\n",
      "F1-score (classe 1): 0.5681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92      1780\n",
      "           1       0.63      0.52      0.57       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.76      0.73      0.74      2161\n",
      "weighted avg       0.85      0.86      0.86      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: LightGBM avec un score de 0.5681\n",
      "\n",
      "=== Test du modèle distilbert-base-uncased (embedding par: mean) ===\n",
      "Création des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings distilbert-base-uncased: 100%|██████████| 8642/8642 [05:36<00:00, 25.69it/s]\n",
      "Embeddings distilbert-base-uncased: 100%|██████████| 2161/2161 [01:24<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle LogisticRegression...\n",
      "Résultats pour LogisticRegression:\n",
      "Exactitude: 0.8283\n",
      "Recall (général): 0.8283\n",
      "F1-score (général): 0.8412\n",
      "Recall (classe 1): 0.7795\n",
      "F1-score (classe 1): 0.6155\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89      1780\n",
      "           1       0.51      0.78      0.62       381\n",
      "\n",
      "    accuracy                           0.83      2161\n",
      "   macro avg       0.73      0.81      0.75      2161\n",
      "weighted avg       0.87      0.83      0.84      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle SVM...\n",
      "Résultats pour SVM:\n",
      "Exactitude: 0.8431\n",
      "Recall (général): 0.8431\n",
      "F1-score (général): 0.8538\n",
      "Recall (classe 1): 0.7874\n",
      "F1-score (classe 1): 0.6390\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      1780\n",
      "           1       0.54      0.79      0.64       381\n",
      "\n",
      "    accuracy                           0.84      2161\n",
      "   macro avg       0.74      0.82      0.77      2161\n",
      "weighted avg       0.88      0.84      0.85      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:28:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour XGBoost:\n",
      "Exactitude: 0.8792\n",
      "Recall (général): 0.8792\n",
      "F1-score (général): 0.8708\n",
      "Recall (classe 1): 0.5118\n",
      "F1-score (classe 1): 0.5991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93      1780\n",
      "           1       0.72      0.51      0.60       381\n",
      "\n",
      "    accuracy                           0.88      2161\n",
      "   macro avg       0.81      0.73      0.76      2161\n",
      "weighted avg       0.87      0.88      0.87      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour LightGBM:\n",
      "Exactitude: 0.8741\n",
      "Recall (général): 0.8741\n",
      "F1-score (général): 0.8755\n",
      "Recall (classe 1): 0.6719\n",
      "F1-score (classe 1): 0.6531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1780\n",
      "           1       0.64      0.67      0.65       381\n",
      "\n",
      "    accuracy                           0.87      2161\n",
      "   macro avg       0.78      0.79      0.79      2161\n",
      "weighted avg       0.88      0.87      0.88      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: LightGBM avec un score de 0.6531\n",
      "\n",
      "=== Test du modèle distilbert-base-uncased (embedding par: cls) ===\n",
      "Création des embeddings (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings distilbert-base-uncased: 100%|██████████| 8642/8642 [06:10<00:00, 23.33it/s]\n",
      "Embeddings distilbert-base-uncased: 100%|██████████| 2161/2161 [01:29<00:00, 24.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle LogisticRegression...\n",
      "Résultats pour LogisticRegression:\n",
      "Exactitude: 0.8390\n",
      "Recall (général): 0.8390\n",
      "F1-score (général): 0.8492\n",
      "Recall (classe 1): 0.7559\n",
      "F1-score (classe 1): 0.6234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90      1780\n",
      "           1       0.53      0.76      0.62       381\n",
      "\n",
      "    accuracy                           0.84      2161\n",
      "   macro avg       0.74      0.81      0.76      2161\n",
      "weighted avg       0.87      0.84      0.85      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle SVM...\n",
      "Résultats pour SVM:\n",
      "Exactitude: 0.8366\n",
      "Recall (général): 0.8366\n",
      "F1-score (général): 0.8479\n",
      "Recall (classe 1): 0.7717\n",
      "F1-score (classe 1): 0.6249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90      1780\n",
      "           1       0.53      0.77      0.62       381\n",
      "\n",
      "    accuracy                           0.84      2161\n",
      "   macro avg       0.74      0.81      0.76      2161\n",
      "weighted avg       0.87      0.84      0.85      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:38:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour XGBoost:\n",
      "Exactitude: 0.8783\n",
      "Recall (général): 0.8783\n",
      "F1-score (général): 0.8698\n",
      "Recall (classe 1): 0.5092\n",
      "F1-score (classe 1): 0.5960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93      1780\n",
      "           1       0.72      0.51      0.60       381\n",
      "\n",
      "    accuracy                           0.88      2161\n",
      "   macro avg       0.81      0.73      0.76      2161\n",
      "weighted avg       0.87      0.88      0.87      2161\n",
      "\n",
      "\n",
      "Entraînement du modèle LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour LightGBM:\n",
      "Exactitude: 0.8649\n",
      "Recall (général): 0.8649\n",
      "F1-score (général): 0.8654\n",
      "Recall (classe 1): 0.6273\n",
      "F1-score (classe 1): 0.6208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      1780\n",
      "           1       0.61      0.63      0.62       381\n",
      "\n",
      "    accuracy                           0.86      2161\n",
      "   macro avg       0.77      0.77      0.77      2161\n",
      "weighted avg       0.87      0.86      0.87      2161\n",
      "\n",
      "\n",
      "Meilleur classifieur: SVM avec un score de 0.6249\n",
      "\n",
      "=== Résumé des résultats ===\n",
      "                    Modèle BERT Stratégie Classifieur  Recall (classe 1)  \\\n",
      "2       distilbert-base-uncased      mean    LightGBM           0.671916   \n",
      "3       distilbert-base-uncased       cls         SVM           0.771654   \n",
      "0  bert-base-multilingual-cased      mean         SVM           0.790026   \n",
      "1  bert-base-multilingual-cased       cls    LightGBM           0.519685   \n",
      "\n",
      "   F1 (classe 1)     Score  \n",
      "2       0.653061  0.653061  \n",
      "3       0.624867  0.624867  \n",
      "0       0.623188  0.623188  \n",
      "1       0.568149  0.568149  \n",
      "\n",
      "Meilleure configuration: distilbert-base-uncased_mean avec LightGBM\n",
      "\n",
      "Création des embeddings pour l'optimisation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings distilbert-base-uncased: 100%|██████████| 8642/8642 [06:14<00:00, 23.09it/s]\n",
      "Embeddings distilbert-base-uncased: 100%|██████████| 2161/2161 [01:34<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimisation du classifieur LightGBM ===\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[LightGBM] [Info] Number of positive: 1524, number of negative: 7118\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 8642, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Meilleurs paramètres: {'learning_rate': 0.3, 'n_estimators': 200, 'num_leaves': 50}\n",
      "\n",
      "Résultats après optimisation:\n",
      "Exactitude: 0.8903\n",
      "Recall (classe 1): 0.5669\n",
      "F1-score (classe 1): 0.6457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      1780\n",
      "           1       0.75      0.57      0.65       381\n",
      "\n",
      "    accuracy                           0.89      2161\n",
      "   macro avg       0.83      0.76      0.79      2161\n",
      "weighted avg       0.88      0.89      0.88      2161\n",
      "\n",
      "\n",
      "Exportation du modèle au format pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\AS3 SALIMOU\\SEMESTRE 2\\NLP\\Tweets_catastrophes\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle exporté: bert_model.pkl\n"
     ]
    }
   ],
   "source": [
    "class BERTEmbedder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Classe pour transformer des textes en embeddings BERT\n",
    "    Compatible avec les pipelines sklearn\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='bert-base-multilingual-cased', max_length=128, embedding_strategy=\"cls\"):\n",
    "        \"\"\"\n",
    "        Initialise la classe BERTEmbedder\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nom du modèle à utiliser\n",
    "            max_length (int): Longueur maximale des séquences après tokenization\n",
    "            embedding_strategy (str): Stratégie d'embedding (\"cls\" ou \"mean\")\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.embedding_strategy = embedding_strategy\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        valid_emb_strategy = (\"cls\", \"mean\")\n",
    "        if self.embedding_strategy not in valid_emb_strategy:\n",
    "            raise ValueError(f\"embedding_strategy doit être l'un des suivants: {valid_emb_strategy}\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Charge le modèle et le tokenizer\n",
    "        Args:\n",
    "            X: Les textes d'entrée\n",
    "            y: Les labels\n",
    "            \n",
    "        Returns:\n",
    "            self: Retourne l'instance pour le chaînage\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.model.eval()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforme les textes en embeddings\n",
    "        \n",
    "        Args:\n",
    "            X: Liste de textes à transformer\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Matrice des embeddings\n",
    "        \"\"\"\n",
    "        # Convertir en liste si nécessaire\n",
    "        if isinstance(X, pd.Series) or isinstance(X, np.ndarray):\n",
    "            X = X.tolist()\n",
    "        elif not isinstance(X, list):\n",
    "            raise ValueError(\"Les données d'entrée doivent être une liste ou convertible en liste de chaînes de caractères.\")\n",
    "        \n",
    "        # Nettoyage et conversion en string\n",
    "        X_cleaned = [str(text) if not pd.isna(text) else \"\" for text in X]\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        for text in tqdm(X_cleaned, desc=f\"Embeddings {self.model_name}\"):\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                                    padding=True, max_length=self.max_length)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            \n",
    "            if self.embedding_strategy == 'cls':\n",
    "                embedding = last_hidden_states[:, 0, :].squeeze().numpy()\n",
    "            else:  # mean\n",
    "                embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "def test_multiple_classifiers(model_name, twenty_train, twenty_test, embedding_strategy=\"cls\"):\n",
    "    \"\"\"\n",
    "    Crée des pipelines sklearn avec BERTEmbedder et différents classifieurs,\n",
    "    les entraîne et les évalue\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Nom du modèle BERT à utiliser\n",
    "        twenty_train: Données d'entraînement\n",
    "        twenty_test: Données de test\n",
    "        embedding_strategy (str): Stratégie d'embedding (\"cls\" ou \"mean\")\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (meilleur_classifieur, scores)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Test du modèle {model_name} (embedding par: {embedding_strategy}) ===\")\n",
    "    \n",
    "    # Assurer que processed_text est une liste de strings\n",
    "    X_train = twenty_train.processed_text.tolist() if hasattr(twenty_train, 'processed_text') else twenty_train.tolist()\n",
    "    X_test = twenty_test.processed_text.tolist() if hasattr(twenty_test, 'processed_text') else twenty_test.tolist()\n",
    "    \n",
    "    y_train = twenty_train.target if hasattr(twenty_train, 'target') else twenty_train\n",
    "    y_test = twenty_test.target if hasattr(twenty_test, 'target') else twenty_test\n",
    "    \n",
    "    # Créer l'embedder BERT\n",
    "    embedder = BERTEmbedder(model_name=model_name, max_length=128, embedding_strategy=embedding_strategy)\n",
    "    \n",
    "    # Transformer les données une seule fois pour éviter de répéter cette opération coûteuse\n",
    "    print(\"Création des embeddings (peut prendre du temps)...\")\n",
    "    X_train_embedded = embedder.fit_transform(X_train)\n",
    "    X_test_embedded = embedder.transform(X_test)\n",
    "    \n",
    "    # Définir les classifieurs à tester\n",
    "    classifiers = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "        'SVM': SVC(class_weight='balanced', random_state=42, probability=True),\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'LightGBM': lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Tester chaque classifieur\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nEntraînement du modèle {name}...\")\n",
    "        clf.fit(X_train_embedded, y_train)\n",
    "        y_pred = clf.predict(X_test_embedded)\n",
    "        \n",
    "        # Évaluer les performances avec focus sur la classe 1\n",
    "        accuracy = (y_pred == y_test).mean()\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall_class1 = recall_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0\n",
    "        f1_class1 = f1_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'recall_class1': recall_class1,\n",
    "            'f1_class1': f1_class1,\n",
    "            'clf': clf\n",
    "        }\n",
    "        \n",
    "        print(f\"Résultats pour {name}:\")\n",
    "        print(f\"Exactitude: {accuracy:.4f}\")\n",
    "        print(f\"Recall (général): {recall:.4f}\")\n",
    "        print(f\"F1-score (général): {f1:.4f}\")\n",
    "        print(f\"Recall (classe 1): {recall_class1:.4f}\")\n",
    "        print(f\"F1-score (classe 1): {f1_class1:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Déterminer le meilleur classifieur en fonction du F1-score et recall pour classe 1\n",
    "    best_score = 0\n",
    "    best_classifier = None\n",
    "    \n",
    "    for name, scores in results.items():\n",
    "        # Score combiné donnant un poids égal au recall et F1 pour la classe 1\n",
    "        score = (scores['f1_class1'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_classifier = name\n",
    "    \n",
    "    print(f\"\\nMeilleur classifieur: {best_classifier} avec un score de {best_score:.4f}\")\n",
    "    \n",
    "    # Créer une pipeline complète avec le meilleur classifieur\n",
    "    best_pipeline = Pipeline([\n",
    "        ('embedder', embedder),\n",
    "        ('classifier', results[best_classifier]['clf'])\n",
    "    ])\n",
    "    \n",
    "    return best_classifier, results, best_pipeline, embedder\n",
    "\n",
    "def optimize_best_classifier(best_classifier, X_train_embedded, y_train, X_test_embedded, y_test):\n",
    "    \"\"\"\n",
    "    Optimise les hyperparamètres du meilleur classifieur\n",
    "    \n",
    "    Args:\n",
    "        best_classifier (str): Nom du meilleur classifieur\n",
    "        X_train_embedded: Données d'entraînement embedées\n",
    "        y_train: Labels d'entraînement\n",
    "        X_test_embedded: Données de test embedées\n",
    "        y_test: Labels de test\n",
    "        \n",
    "    Returns:\n",
    "        model: Modèle optimisé\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Optimisation du classifieur {best_classifier} ===\")\n",
    "    \n",
    "    # Définir les grilles de paramètres pour chaque classifieur\n",
    "    param_grids = {\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear', 'lbfgs'],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Sélectionner le classifieur et la grille de paramètres appropriés\n",
    "    if best_classifier == 'LogisticRegression':\n",
    "        clf = LogisticRegression(class_weight='balanced', random_state=42, max_iter=2000)\n",
    "    elif best_classifier == 'SVM':\n",
    "        clf = SVC(class_weight='balanced', random_state=42, probability=True)\n",
    "    elif best_classifier == 'XGBoost':\n",
    "        clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    elif best_classifier == 'LightGBM':\n",
    "        clf = lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "    else:\n",
    "        raise ValueError(f\"Classifieur {best_classifier} non reconnu\")\n",
    "    \n",
    "    param_grid = param_grids[best_classifier]\n",
    "    \n",
    "    # Optimisation par validation croisée\n",
    "    grid_search = GridSearchCV(\n",
    "        clf, param_grid,\n",
    "        cv=3,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_embedded, y_train)\n",
    "    \n",
    "    # Récupérer le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Évaluer le modèle optimisé\n",
    "    y_pred = best_model.predict(X_test_embedded)\n",
    "    \n",
    "    print(\"\\nRésultats après optimisation:\")\n",
    "    print(f\"Exactitude: {(y_pred == y_test).mean():.4f}\")\n",
    "    print(f\"Recall (classe 1): {recall_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0:.4f}\")\n",
    "    print(f\"F1-score (classe 1): {f1_score(y_test, y_pred, average=None)[1] if 1 in np.unique(y_test) else 0:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def run_bert_experiments(twenty_train, twenty_test):\n",
    "    \"\"\"\n",
    "    Exécute des expériences avec différents modèles BERT, stratégies d'embedding\n",
    "    et différents classifieurs\n",
    "    \n",
    "    Args:\n",
    "        twenty_train: Données d'entraînement\n",
    "        twenty_test: Données de test\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (meilleur_modèle, meilleur_embedder)\n",
    "    \"\"\"\n",
    "    models = [\n",
    "        'bert-base-multilingual-cased',  # BERT multilingue (pour plusieurs langues)\n",
    "        'distilbert-base-uncased',       # DistilBERT (version plus légère)\n",
    "    ]\n",
    "    \n",
    "    strategies = ['mean', 'cls']  # Stratégies d'embedding: 'mean' ou 'cls'\n",
    "    \n",
    "    results = {}\n",
    "    best_pipelines = {}\n",
    "    \n",
    "    for model in models:\n",
    "        for strategy in strategies:\n",
    "            model_key = f\"{model}_{strategy}\"\n",
    "            best_clf, clf_results, pipeline, embedder = test_multiple_classifiers(\n",
    "                model, twenty_train, twenty_test, embedding_strategy=strategy\n",
    "            )\n",
    "            \n",
    "            # Stocker les résultats\n",
    "            results[model_key] = {\n",
    "                'best_classifier': best_clf,\n",
    "                'classifier_results': clf_results\n",
    "            }\n",
    "            \n",
    "            best_pipelines[model_key] = pipeline\n",
    "    \n",
    "    # Déterminer la meilleure combinaison (modèle BERT + stratégie + classifieur)\n",
    "    best_score = 0\n",
    "    best_config = None\n",
    "    \n",
    "    print(\"\\n=== Résumé des résultats ===\")\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        model, strategy = model_key.split('_')\n",
    "        best_clf = result['best_classifier']\n",
    "        clf_scores = result['classifier_results'][best_clf]\n",
    "        \n",
    "        # Score combiné (recall + f1 pour classe 1)\n",
    "        score = (clf_scores['f1_class1'])\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Modèle BERT': model,\n",
    "            'Stratégie': strategy,\n",
    "            'Classifieur': best_clf,\n",
    "            'Recall (classe 1)': clf_scores['recall_class1'],\n",
    "            'F1 (classe 1)': clf_scores['f1_class1'],\n",
    "            'Score': score\n",
    "        })\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_config = model_key\n",
    "    \n",
    "    # Afficher un résumé des résultats\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.sort_values('Score', ascending=False))\n",
    "    \n",
    "    if best_config:\n",
    "        print(f\"\\nMeilleure configuration: {best_config} avec {results[best_config]['best_classifier']}\")\n",
    "        best_model_key = best_config\n",
    "        best_clf_name = results[best_config]['best_classifier']\n",
    "        \n",
    "        # Préparer les données pour l'optimisation\n",
    "        model_name, embedding_strategy = best_model_key.split('_')\n",
    "        embedder = BERTEmbedder(model_name=model_name, max_length=128, embedding_strategy=embedding_strategy)\n",
    "        \n",
    "        # Transformer les données\n",
    "        X_train = twenty_train.processed_text.tolist() if hasattr(twenty_train, 'processed_text') else twenty_train.tolist()\n",
    "        X_test = twenty_test.processed_text.tolist() if hasattr(twenty_test, 'processed_text') else twenty_test.tolist()\n",
    "        y_train = twenty_train.target if hasattr(twenty_train, 'target') else twenty_train\n",
    "        y_test = twenty_test.target if hasattr(twenty_test, 'target') else twenty_test\n",
    "        \n",
    "        print(\"\\nCréation des embeddings pour l'optimisation...\")\n",
    "        X_train_embedded = embedder.fit_transform(X_train)\n",
    "        X_test_embedded = embedder.transform(X_test)\n",
    "        \n",
    "        # Optimiser le meilleur classifieur\n",
    "        best_model = optimize_best_classifier(\n",
    "            best_clf_name, X_train_embedded, y_train, X_test_embedded, y_test\n",
    "        )\n",
    "        \n",
    "        # Créer la pipeline finale\n",
    "        final_pipeline = Pipeline([\n",
    "            ('embedder', embedder),\n",
    "            ('classifier', best_model)\n",
    "        ])\n",
    "\n",
    "        # Exporter le modèle\n",
    "        print(\"\\nExportation du modèle au format pkl...\")\n",
    "        with open(f'bert_model.pkl', 'wb') as f:\n",
    "            pickle.dump(final_pipeline, f)\n",
    "        print(f\"Modèle exporté: bert_model.pkl\")\n",
    "        \n",
    "        return final_pipeline, embedder\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Pour exécuter les expériences:\n",
    "final_pipeline, embedder = run_bert_experiments(X_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
